{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Regression (Exercise)\n",
    "\n",
    "This dataset encompasses details about various workers and their corresponding employment levels, featuring a diverse set of attributes ranging from categorical to continuous. Initialize the data loading process using the suitable Pandas function, and meticulously inspect for any instances of null or duplicated data. Specifically focusing on the **“salary in usd”** feature, identify and eliminate outliers while devising a strategy to address any missing values.\n",
    "\n",
    "Then, use any method you like to encode the categorical features, namely **“work year”, “experience level”, “employment type”, “job title”, “employee residence”, “remote ratio”, “company location”, and “company size”**. You may consider to employ the sklearn LabelEncoder class<sup>1</sup>.\n",
    "\n",
    "Following the preprocessing steps, normalize the dataset utilizing the z-score technique to ensure consistent scaling across features. Subsequently, construct a neural network using **PyTorch**, incorporating **2 hidden layers with 5 and 3 neurons**, respectively. Carefully select an appropriate learning rate and normalization value for optimal model training.\n",
    "\n",
    "Furthermore, assess the model’s performance using a relevant evaluation metric, ensuring a comprehensive understanding of its effectiveness in handling the given employment dataset. Finally, find the best hyperparameter combination (namely **lr** and **weight decay**) using both the **Grid Search** and the **k-fold cross validation** methods.\n",
    "\n",
    "<sup>1</sup>https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Version**\n",
    "\n",
    "Here, I first present a simpler implementation that strictly follows the track's instructions and utilizes Sklearn's GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Software Engineer</td>\n",
       "      <td>168000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>179975</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>144000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Applied Scientist</td>\n",
       "      <td>222200</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>EX</td>\n",
       "      <td>FT</td>\n",
       "      <td>Head of Data</td>\n",
       "      <td>230000</td>\n",
       "      <td>RU</td>\n",
       "      <td>50</td>\n",
       "      <td>RU</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>180180</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>2023</td>\n",
       "      <td>EX</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>310000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>2021</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>62649</td>\n",
       "      <td>FR</td>\n",
       "      <td>50</td>\n",
       "      <td>FR</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>2022</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>84053</td>\n",
       "      <td>FR</td>\n",
       "      <td>100</td>\n",
       "      <td>DE</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year experience_level employment_type  \\\n",
       "0          2022               SE              FT   \n",
       "1          2023               SE              FT   \n",
       "2          2022               SE              FT   \n",
       "3          2023               SE              FT   \n",
       "4          2021               EX              FT   \n",
       "...         ...              ...             ...   \n",
       "3750       2023               SE              FT   \n",
       "3751       2023               SE              FT   \n",
       "3752       2023               EX              FT   \n",
       "3753       2021               MI              FT   \n",
       "3754       2022               MI              FT   \n",
       "\n",
       "                               job_title  salary_in_usd employee_residence  \\\n",
       "0     Machine Learning Software Engineer         168000                 CA   \n",
       "1                           Data Analyst         179975                 US   \n",
       "2                         Data Scientist         144000                 US   \n",
       "3                      Applied Scientist         222200                 US   \n",
       "4                           Head of Data         230000                 RU   \n",
       "...                                  ...            ...                ...   \n",
       "3750           Machine Learning Engineer         150000                 US   \n",
       "3751                        Data Analyst         180180                 US   \n",
       "3752                       Data Engineer         310000                 US   \n",
       "3753                  Research Scientist          62649                 FR   \n",
       "3754           Machine Learning Engineer          84053                 FR   \n",
       "\n",
       "      remote_ratio company_location company_size  \n",
       "0              100               CA            M  \n",
       "1              100               US            M  \n",
       "2              100               US            M  \n",
       "3                0               US            L  \n",
       "4               50               RU            L  \n",
       "...            ...              ...          ...  \n",
       "3750           100               US            M  \n",
       "3751             0               US            M  \n",
       "3752           100               US            M  \n",
       "3753            50               FR            M  \n",
       "3754           100               DE            M  \n",
       "\n",
       "[3755 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv('./datasets/ds_salaries.csv')\n",
    "# Here I choose to delete some redundant features such as salary and salary_currency since thay are too similar to our target variable 'salary_in_usd'\n",
    "df.drop(['salary', 'salary_currency'], axis=1, inplace=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values: 0\n",
      "\n",
      "Duplicates: 1171\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\", df.isnull().sum().sum())\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 9)\n"
     ]
    }
   ],
   "source": [
    "# In this case we found that we don't have missing values. However in general if would have some of them we could handle it as follows:\n",
    "# - for a Numerical feature we could think to use the median of its values\n",
    "# - for a Categorical feature we could think to use the most frequent value (i.e. the mode of its values)\n",
    "\n",
    "# For what regard the duplicates we have 1171 duplicates ... we can drop them\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2555, 9)\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove outliers in \"salary in usd\" using IQR\n",
    "Q1 = df['salary_in_usd'].quantile(0.25)\n",
    "Q3 = df['salary_in_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# we take just entries within the range [lower_bound, upper_bound]\n",
    "df = df[(df['salary_in_usd'] >= lower_bound) & (df['salary_in_usd'] <= upper_bound)]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding of Categorical Features**\n",
    "\n",
    "To apply **standardization**, all features must be numeric. While continuous features are already numerical, categorical features must first be **transformed into numerical values**. This transformation process is called **encoding**. One way we can encode categorical features is using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['work_year', 'salary_in_usd', 'remote_ratio'], dtype='object')\n",
      "Index(['experience_level', 'employment_type', 'job_title',\n",
      "       'employee_residence', 'company_location', 'company_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = df.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "print(numerical_features)\n",
    "print(categorical_features)\n",
    "\n",
    "# Encoding of the categorical features\n",
    "le = LabelEncoder()\n",
    "for cat in categorical_features:\n",
    "    df[cat] = le.fit_transform(df[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('salary_in_usd', axis=1).values\n",
    "y = df['salary_in_usd'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalization with z-score using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) # Input -> First Hidden Layer\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) # First Hidden Layer layer -> Second Hidden Layer\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size) # Second Hidden Layer -> Output Layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x) # In regression task the sigmoid is not applied at the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Scikit-Learn’s `GridSearchCV`, we cannot directly pass a PyTorch model as it does not conform to Scikit-Learn’s standardized estimator interface. `GridSearchCV` expects an estimator that implements  at least the `fit` and `predict` methods following the Scikit-Learn API.\n",
    "To make our PyTorch neural network compatible with Scikit-Learn, we need to create a wrapper class, `PytorchNN`, which encapsulates our model and inherits from `BaseEstimator` and `TransformerMixin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchNN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1, weight_decay=0.001, epochs=1000):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.simple_nn = RegressionNN(self.input_size, self.hidden_size, self.output_size)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(self.simple_nn.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.simple_nn.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.simple_nn(X_tensor)\n",
    "            loss = criterion(output, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.simple_nn.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            output = self.simple_nn(X_tensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we can easily integrate the neural network within Scikit-Learn workflows, enabling us to use `GridSearchCV` for hyperparameter optimization.\n",
    "\n",
    "Another important point to note is that Scikit-Learn's GridSearchCV operates by **maximizing** the metric specified in the `scoring` parameter. However, in regression tasks, our goal is to minimize the Mean Squared Error (MSE). To align with Scikit-Learn's implementation, we should use `neg_mean_squared_error`, meaning we **maximize the negative MSE** instead. This ensures proper optimization during hyperparameter tuning. You can find a full list of available evaluation metrics at [Model Evaluation - Scoring Parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "\n",
      "Best Score: \n",
      " 20809608240.26951\n",
      "\n",
      "Best Hyperparameters: \n",
      " {'lr': 0.1, 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'weight_decay': [0.0, 0.1, 0.01]\n",
    "}\n",
    "\n",
    "model = PytorchNN(input_size=X.shape[1], hidden_size=[5, 3], output_size=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=True)\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and best params\n",
    "print(\"\\nBest Score: \\n\", - grid_search.best_score_)\n",
    "print(\"\\nBest Hyperparameters: \\n\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2125],\n",
      "        [404.2124]])\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Version**\n",
    "\n",
    "Now, let's explore a different implementation that highlights other aspects, such as best practices for encoding. Additionally, since Scikit-Learn only supports CPU, while PyTorch can leverage both CPU and GPU, it is often more efficient to handle everything directly in PyTorch. Therefore, we will also implement GridSearch with K-Fold cross-validation from **scratch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding of Categorical Features**\n",
    "\n",
    "Above we used the Label Encoder to encode all categorical variables as the track recommended, but I can tell you that it's not a good move. LabelEncoder is designed to encode target labels with value between 0 and n_classes-1, therefore it make no sense to use it for features encoding. Moreover, when encoding features is important to understand the typology of feature considered, since different typologies lead to [different encodings](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features).\n",
    "\n",
    "Specifically, categorical features fall into two main categories:\n",
    "1. **Ordinal Features**: Have a meaningful order (e.g., education level: *High School < Bachelor's < Master's < PhD*).\n",
    "2. **Nominal Features**: Have no intrinsic order (e.g.company location).\n",
    "\n",
    "**1️⃣ Ordinal Features → Ordinal Encoding**\n",
    "\n",
    "Assigns numerical values based on category order. For example, education levels may be assigned values such as:\n",
    "- High School → 0\n",
    "- Bachelor's → 1\n",
    "- Master's → 2\n",
    "- PhD → 3\n",
    "\n",
    "Since ordinal variables have a meaningful ranking, converting them into ordered numbers preserves that relationship.\n",
    "\n",
    "**2️⃣ Nominal Features → One-Hot Encoding (OHE)**\n",
    "\n",
    "Creates binary columns for each category. For instance, if a feature represents company size with values **S, M, L**, one-hot encoding transforms it into three separate columns:\n",
    "- `S` → [1, 0, 0]\n",
    "- `M` → [0, 1, 0]\n",
    "- `L` → [0, 0, 1]\n",
    "\n",
    "One-hot encoding ensures that categories are represented equally without implying any order.\n",
    "\n",
    "⚠️ **Issue: High Cardinality in Nominal Features**\n",
    "For categorical features with many unique values (e.g., thousands of job titles), one-hot encoding creates too many columns, which could lead to the curse of dimensionality.\n",
    "A better alternative in such a case is **Target Encoding**.\n",
    "\n",
    "**3️⃣ Alternative for High-Cardinality Nominal Features → Target Encoding**\n",
    "\n",
    "Replaces each category with the **mean target value** (e.g., average salary for each job title).\n",
    "- Example: If the average salary for “Software Engineer” is 80,000 USD and for “Data Scientist” is 90,000 USD, we replace:\n",
    "Software Engineer → 80,000\n",
    "Data Scientist → 90,000\n",
    "\n",
    "It solve the one-hot encoding **dimensionality explosion** problem (i.e. when dealing with features that have a large number of unique values) while keeping useful statistical information about the relationship between a category and the target variable.\n",
    "\n",
    "##### **When to Apply Encoding?**\n",
    "Just like standardization, encoding should be computed only on the training set:\n",
    "- Fit and transform on the training data (fit_transform).\n",
    "- Transform only on the test set (transform).\n",
    "\n",
    "⚠️ In general, always rember that the test set should remain **untouched**, mimicking real-world data the model has never seen before.\n",
    "\n",
    "Once all features are numeric (including encoded categorical ones), we can apply Z-score standardization to all features (both the transformed categorical ones and the original continuous numerical features).\n",
    "\n",
    "This ensures that all features are on the same scale, having a mean of 0 and a standard deviation of 1, helping models like neural networks learn efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values: 0\n",
      "\n",
      "Duplicates: 1171\n",
      "\n",
      " (2555, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/ds_salaries.csv')\n",
    "df.drop(['salary', 'salary_currency'], axis=1, inplace=True)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check for missing values -> NO missing values\n",
    "print(\"\\nMissing Values:\", df.isnull().sum().sum())\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicates:\", df.duplicated().sum())\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Identify and remove outliers in \"salary in usd\" using IQR\n",
    "alpha = 1.5\n",
    "Q1 = df['salary_in_usd'].quantile(0.25)\n",
    "Q3 = df['salary_in_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - alpha * IQR\n",
    "upper_bound = Q3 + alpha * IQR\n",
    "\n",
    "# we take just entries within the range [lower_bound, upper_bound]\n",
    "df = df[(df['salary_in_usd'] >= lower_bound) & (df['salary_in_usd'] <= upper_bound)]\n",
    "\n",
    "print(\"\\n\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('salary_in_usd', axis=1)\n",
    "y = df['salary_in_usd']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle categorical features we will proceed as follows: we first distinguish between ordinal and nominal features. \n",
    "\n",
    "For the **ordinal features** we use a Ordinal Encoder, instead for the **nominal features** if the number of unique values is fewer than 10 we apply **One-Hot Encoding**, otherwise we apply **Target Encoding** (to reduce dimensionality explosion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding features:  ['employment_type']\n",
      "Target encoding features:  ['job_title', 'employee_residence', 'company_location']\n"
     ]
    }
   ],
   "source": [
    "ordinal_features = ['experience_level', 'company_size']\n",
    "nominal_features = ['employment_type', 'job_title', 'employee_residence', 'company_location']\n",
    "\n",
    "one_hot_features = []  # To be filled dynamically\n",
    "target_features = []   # To be filled dynamically \n",
    "\n",
    "# For ordinal features, you can simply use OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[ordinal_features] = ordinal_encoder.fit_transform(X_train[ordinal_features])\n",
    "X_test[ordinal_features] = ordinal_encoder.transform(X_test[ordinal_features])\n",
    "\n",
    "\n",
    "threshold = 10 # max unique values\n",
    "# Let's handle nominal features features\n",
    "for col in nominal_features:\n",
    "    if X_train[col].nunique() <= threshold:\n",
    "        one_hot_features.append(col)  # If number of unique values is below threshold, use one-hot encoding\n",
    "    else:\n",
    "        target_features.append(col)  # Otherwise, use target encoding\n",
    "# Note: Basically here we will one_hot_features and target_features based on the number of unique values\n",
    "\n",
    "print(\"One-hot encoding features: \", one_hot_features)\n",
    "print(\"Target encoding features: \", target_features)\n",
    "\n",
    "# Apply One-Hot Encoding for the `one_hot_features`\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, dtype=float, handle_unknown='ignore')\n",
    "X_train_onehot = onehot_encoder.fit_transform(X_train[one_hot_features])\n",
    "X_test_onehot = onehot_encoder.transform(X_test[one_hot_features])\n",
    "X_train_onehot_df = pd.DataFrame(X_train_onehot, \n",
    "                                 columns=onehot_encoder.get_feature_names_out(one_hot_features), \n",
    "                                 index=X_train.index)\n",
    "X_test_onehot_df = pd.DataFrame(X_test_onehot, \n",
    "                                columns=onehot_encoder.get_feature_names_out(one_hot_features), \n",
    "                                index=X_test.index)\n",
    "X_train = pd.concat([X_train, X_train_onehot_df], axis=1).drop(columns=one_hot_features)\n",
    "X_test = pd.concat([X_test, X_test_onehot_df], axis=1).drop(columns=one_hot_features)\n",
    "\n",
    "# Apply Target Encoding for the `target_features`\n",
    "target_encoder = TargetEncoder()\n",
    "X_train[target_features] = target_encoder.fit_transform(X_train[target_features], y_train)\n",
    "X_test[target_features] = target_encoder.transform(X_test[target_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>job_title</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>employment_type_CT</th>\n",
       "      <th>employment_type_FL</th>\n",
       "      <th>employment_type_FT</th>\n",
       "      <th>employment_type_PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>2023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138032.720430</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>0</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115766.207915</td>\n",
       "      <td>41918.604303</td>\n",
       "      <td>100</td>\n",
       "      <td>38347.912811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>2023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>163090.452572</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>100</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>2023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104597.610592</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>100</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>2022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>172206.957588</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>0</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2023</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130669.230047</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>0</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>2022</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130669.230047</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>0</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>2023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142020.375994</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>0</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>2022</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104597.610592</td>\n",
       "      <td>150378.151839</td>\n",
       "      <td>100</td>\n",
       "      <td>148754.081258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2022</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129359.995145</td>\n",
       "      <td>127346.199065</td>\n",
       "      <td>100</td>\n",
       "      <td>126694.824621</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2044 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year  experience_level      job_title  employee_residence  \\\n",
       "3749       2023               3.0  138032.720430       150378.151839   \n",
       "3551       2021               0.0  115766.207915        41918.604303   \n",
       "2617       2023               3.0  163090.452572       150378.151839   \n",
       "2596       2023               3.0  104597.610592       150378.151839   \n",
       "3345       2022               1.0  172206.957588       150378.151839   \n",
       "...         ...               ...            ...                 ...   \n",
       "2156       2023               3.0  130669.230047       150378.151839   \n",
       "1325       2022               2.0  130669.230047       150378.151839   \n",
       "1378       2023               2.0  142020.375994       150378.151839   \n",
       "1623       2022               3.0  104597.610592       150378.151839   \n",
       "1007       2022               2.0  129359.995145       127346.199065   \n",
       "\n",
       "      remote_ratio  company_location  company_size  employment_type_CT  \\\n",
       "3749             0     148754.081258           1.0                 0.0   \n",
       "3551           100      38347.912811           1.0                 0.0   \n",
       "2617           100     148754.081258           1.0                 0.0   \n",
       "2596           100     148754.081258           1.0                 0.0   \n",
       "3345             0     148754.081258           1.0                 0.0   \n",
       "...            ...               ...           ...                 ...   \n",
       "2156             0     148754.081258           1.0                 0.0   \n",
       "1325             0     148754.081258           1.0                 0.0   \n",
       "1378             0     148754.081258           1.0                 0.0   \n",
       "1623           100     148754.081258           1.0                 0.0   \n",
       "1007           100     126694.824621           2.0                 0.0   \n",
       "\n",
       "      employment_type_FL  employment_type_FT  employment_type_PT  \n",
       "3749                 0.0                 1.0                 0.0  \n",
       "3551                 0.0                 1.0                 0.0  \n",
       "2617                 0.0                 1.0                 0.0  \n",
       "2596                 0.0                 1.0                 0.0  \n",
       "3345                 0.0                 1.0                 0.0  \n",
       "...                  ...                 ...                 ...  \n",
       "2156                 0.0                 1.0                 0.0  \n",
       "1325                 0.0                 1.0                 0.0  \n",
       "1378                 0.0                 1.0                 0.0  \n",
       "1623                 0.0                 1.0                 0.0  \n",
       "1007                 0.0                 1.0                 0.0  \n",
       "\n",
       "[2044 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform in numpy array\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now all features present numeric values, so as next step we can apply standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93128887,  0.66335   ,  0.3671331 , ..., -0.05425922,\n",
       "         0.11127609, -0.08000492],\n",
       "       [-1.72545146, -2.44886284, -1.02700537, ..., -0.05425922,\n",
       "         0.11127609, -0.08000492],\n",
       "       [ 0.93128887,  0.66335   ,  1.93603386, ..., -0.05425922,\n",
       "         0.11127609, -0.08000492],\n",
       "       ...,\n",
       "       [ 0.93128887, -0.37405428,  0.61680597, ..., -0.05425922,\n",
       "         0.11127609, -0.08000492],\n",
       "       [-0.3970813 ,  0.66335   , -1.72628736, ..., -0.05425922,\n",
       "         0.11127609, -0.08000492],\n",
       "       [-0.3970813 , -0.37405428, -0.17587874, ..., -0.05425922,\n",
       "         0.11127609, -0.08000492]], shape=(2044, 11))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2044,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2044, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's transform y_train and y_test to be column vectors (so they will match output layer of our NN)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[192000],\n",
       "       [ 12171],\n",
       "       [168100],\n",
       "       ...,\n",
       "       [230000],\n",
       "       [135000],\n",
       "       [ 75000]], shape=(2044, 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "y_test_tensor= torch.from_numpy(y_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9313,  0.6633,  0.3671,  ..., -0.0543,  0.1113, -0.0800],\n",
      "        [-1.7255, -2.4489, -1.0270,  ..., -0.0543,  0.1113, -0.0800],\n",
      "        [ 0.9313,  0.6633,  1.9360,  ..., -0.0543,  0.1113, -0.0800],\n",
      "        ...,\n",
      "        [ 0.9313, -0.3741,  0.6168,  ..., -0.0543,  0.1113, -0.0800],\n",
      "        [-0.3971,  0.6633, -1.7263,  ..., -0.0543,  0.1113, -0.0800],\n",
      "        [-0.3971, -0.3741, -0.1759,  ..., -0.0543,  0.1113, -0.0800]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) # Input -> First Hidden Layer\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) # First Hidden Layer layer -> Second Hidden Layer\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size) # Second Hidden Layer -> Output Layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x) # In regression task the sigmoid is not applied at the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 200 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 300 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 400 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 500 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 600 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 700 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 800 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 900 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n",
      "Epoch: 1000 | Training loss (MSE): 3886808064.00 | Test MSE: 4074648832.00, Test MAE: 52242.20\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = [5, 3]\n",
    "output_size = 1\n",
    "model = RegressionNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # SGD optimizer\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train_tensor = model(X_train_tensor)\n",
    "    loss = criterion(preds_train_tensor, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # Evaluate the model on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test_tensor = model(X_test_tensor)\n",
    "            test_mse = criterion(preds_test_tensor, y_test_tensor)\n",
    "            test_mae = mean_absolute_error(preds_test_tensor.cpu(), y_test_tensor.cpu()) # here I switch to CPU just because MAE of sklearn is not able to work on GPU\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {loss.item():.2f} | Test MSE: {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to perform **hyperparameter tuning** using **Grid Search** with **k-fold cross validation** it's efficient to encapsulate the entire process in a single method. This approach allows us to easily repeat the training and evaluation process as needed. Let's create a method to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, X_train_fold_tensor, y_train_fold_tensor, X_val_fold_tensor, y_val_fold_tensor, fold, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train and evaluate a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        criterion (torch.nn.Module): The loss function (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training (e.g., Adam, SGD).\n",
    "        X_train_fold_tensor (torch.Tensor): Training features.\n",
    "        y_train_fold_tensor (torch.Tensor): Training target values.\n",
    "        X_val_fold_tensor (torch.Tensor): Test features.\n",
    "        y_val_fold_tensor (torch.Tensor): Test target values.\n",
    "        fold: Fold number considered (used just for logging)\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        float: Final val loss (MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        # Forward pass\n",
    "        preds_train = model(X_train_fold_tensor)\n",
    "        loss = criterion(preds_train, y_train_fold_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if (epoch+1) % num_epochs == 0: # we print just the last step results\n",
    "            # Evaluate the model on the test data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Compute MSE and MAE for validation data (just for logging)\n",
    "                preds_val = model(X_val_fold_tensor)      \n",
    "                val_mse = criterion(preds_val, y_val_fold_tensor)\n",
    "                val_mae = mean_absolute_error(preds_val.cpu(), y_val_fold_tensor.cpu())\n",
    "            print(f\"  Fold: {fold} | Val loss (MSE): {val_mse.item():.2f}, Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "    return val_mse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are ready to actually perform hyperparameter tuning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits.\n",
      "\n",
      "\n",
      "[1/9] Testing Params: lr=0.1, weight_decay=0\n",
      "  Fold: 1 | Val loss (MSE): 4317376512.00, Val MAE: 53677.58\n",
      "  Fold: 2 | Val loss (MSE): 3936137216.00, Val MAE: 51221.72\n",
      "  Fold: 3 | Val loss (MSE): 3615389184.00, Val MAE: 48164.67\n",
      "  Fold: 4 | Val loss (MSE): 4047974656.00, Val MAE: 50643.90\n",
      "  Fold: 5 | Val loss (MSE): 3539492864.00, Val MAE: 48489.63\n",
      "Avg Loss for params (0.1, 0): 3891274086.40\n",
      "\n",
      "[2/9] Testing Params: lr=0.1, weight_decay=0.1\n",
      "  Fold: 1 | Val loss (MSE): 4344276480.00, Val MAE: 53868.32\n",
      "  Fold: 2 | Val loss (MSE): 3952835584.00, Val MAE: 51267.00\n",
      "  Fold: 3 | Val loss (MSE): 3128572160.00, Val MAE: 44419.73\n",
      "  Fold: 4 | Val loss (MSE): 4066266112.00, Val MAE: 50619.09\n",
      "  Fold: 5 | Val loss (MSE): 3524785664.00, Val MAE: 48228.12\n",
      "Avg Loss for params (0.1, 0.1): 3803347200.00\n",
      "\n",
      "[3/9] Testing Params: lr=0.1, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 4319204352.00, Val MAE: 53691.05\n",
      "  Fold: 2 | Val loss (MSE): 3937418496.00, Val MAE: 51224.36\n",
      "  Fold: 3 | Val loss (MSE): 3613313536.00, Val MAE: 48138.47\n",
      "  Fold: 4 | Val loss (MSE): 4050172416.00, Val MAE: 50638.36\n",
      "  Fold: 5 | Val loss (MSE): 3538242304.00, Val MAE: 48472.76\n",
      "Avg Loss for params (0.1, 0.01): 3891670220.80\n",
      "\n",
      "[4/9] Testing Params: lr=0.01, weight_decay=0\n",
      "  Fold: 1 | Val loss (MSE): 4317377024.00, Val MAE: 53677.58\n",
      "  Fold: 2 | Val loss (MSE): 3936137728.00, Val MAE: 51221.72\n",
      "  Fold: 3 | Val loss (MSE): 3615388672.00, Val MAE: 48164.67\n",
      "  Fold: 4 | Val loss (MSE): 4047975168.00, Val MAE: 50643.90\n",
      "  Fold: 5 | Val loss (MSE): 3539492096.00, Val MAE: 48489.63\n",
      "Avg Loss for params (0.01, 0): 3891274137.60\n",
      "\n",
      "[5/9] Testing Params: lr=0.01, weight_decay=0.1\n",
      "  Fold: 1 | Val loss (MSE): 4333242880.00, Val MAE: 53793.25\n",
      "  Fold: 2 | Val loss (MSE): 3964272896.00, Val MAE: 51305.10\n",
      "  Fold: 3 | Val loss (MSE): 3602264320.00, Val MAE: 47974.98\n",
      "  Fold: 4 | Val loss (MSE): 4126181376.00, Val MAE: 50711.58\n",
      "  Fold: 5 | Val loss (MSE): 3529486080.00, Val MAE: 48335.52\n",
      "Avg Loss for params (0.01, 0.1): 3911089510.40\n",
      "\n",
      "[6/9] Testing Params: lr=0.01, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 4318265344.00, Val MAE: 53684.32\n",
      "  Fold: 2 | Val loss (MSE): 3938092032.00, Val MAE: 51225.68\n",
      "  Fold: 3 | Val loss (MSE): 3612314368.00, Val MAE: 48125.40\n",
      "  Fold: 4 | Val loss (MSE): 4050172416.00, Val MAE: 50638.36\n",
      "  Fold: 5 | Val loss (MSE): 3538242304.00, Val MAE: 48472.77\n",
      "Avg Loss for params (0.01, 0.01): 3891417292.80\n",
      "\n",
      "[7/9] Testing Params: lr=0.001, weight_decay=0\n",
      "  Fold: 1 | Val loss (MSE): 4319162368.00, Val MAE: 53690.76\n",
      "  Fold: 2 | Val loss (MSE): 3955214080.00, Val MAE: 51275.55\n",
      "  Fold: 3 | Val loss (MSE): 3597894144.00, Val MAE: 47907.79\n",
      "  Fold: 4 | Val loss (MSE): 4050121216.00, Val MAE: 50638.48\n",
      "  Fold: 5 | Val loss (MSE): 3537099520.00, Val MAE: 48456.67\n",
      "Avg Loss for params (0.001, 0): 3891898265.60\n",
      "\n",
      "[8/9] Testing Params: lr=0.001, weight_decay=0.1\n",
      "  Fold: 1 | Val loss (MSE): 4328804864.00, Val MAE: 53761.47\n",
      "  Fold: 2 | Val loss (MSE): 3948207360.00, Val MAE: 51251.85\n",
      "  Fold: 3 | Val loss (MSE): 3597836544.00, Val MAE: 47906.84\n",
      "  Fold: 4 | Val loss (MSE): 4069366016.00, Val MAE: 50620.57\n",
      "  Fold: 5 | Val loss (MSE): 3526294272.00, Val MAE: 48111.95\n",
      "Avg Loss for params (0.001, 0.1): 3894101811.20\n",
      "\n",
      "[9/9] Testing Params: lr=0.001, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 4320472064.00, Val MAE: 53699.59\n",
      "  Fold: 2 | Val loss (MSE): 3959007232.00, Val MAE: 51288.45\n",
      "  Fold: 3 | Val loss (MSE): 3768468224.00, Val MAE: 48374.95\n",
      "  Fold: 4 | Val loss (MSE): 4051694336.00, Val MAE: 50634.80\n",
      "  Fold: 5 | Val loss (MSE): 3537926912.00, Val MAE: 48468.39\n",
      "Avg Loss for params (0.001, 0.01): 3927513753.60\n",
      "\n",
      "\n",
      "Best Params: lr=0.1, weight_decay=0.1\n",
      "Best Loss: 3803347200.00\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "cv_folds = 5 # Number of folds\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'weight_decay': [0, 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Convert param_grid to all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "num_candidates = len(param_combinations)  # Total hyperparameter combinations\n",
    "total_fits = num_candidates * cv_folds  # Total model fits\n",
    "\n",
    "print(f\"Fitting {cv_folds} folds for each of {num_candidates} candidates, totalling {total_fits} fits.\\n\")\n",
    "# Iterate over all parameter combinations\n",
    "for idx, params in enumerate(param_combinations):\n",
    "    lr, weight_decay = params\n",
    "    print(f\"\\n[{idx+1}/{num_candidates}] Testing Params: lr={lr}, weight_decay={weight_decay}\")\n",
    "\n",
    "    fold_losses = []\n",
    "    fold=0\n",
    "\n",
    "    # Iteratively consider all fold configurations\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        fold=fold +1\n",
    "        # Split data into train and validation sets for the current fold\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        X_val_fold = X_train[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Convert data to torch tensors\n",
    "        X_train_fold_tensor = torch.from_numpy(X_train_fold).float().to(device)\n",
    "        y_train_fold_tensor = torch.from_numpy(y_train_fold).float().to(device)\n",
    "        X_val_fold_tensor = torch.from_numpy(X_val_fold).float().to(device)\n",
    "        y_val_fold_tensor = torch.from_numpy(y_val_fold).float().to(device)\n",
    "\n",
    "        # Initialize model, optimizer (with the lr and weight decay values to test) and criterion\n",
    "        model = RegressionNN(input_size=X_train_fold_tensor.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        fold_loss = train_and_evaluate(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            X_train_fold_tensor=X_train_fold_tensor,\n",
    "            y_train_fold_tensor=y_train_fold_tensor,\n",
    "            X_val_fold_tensor=X_val_fold_tensor,\n",
    "            y_val_fold_tensor=y_val_fold_tensor,\n",
    "            fold=fold\n",
    "        )\n",
    "        fold_losses.append(fold_loss)\n",
    "\n",
    "    # Calculate average loss across all folds\n",
    "    avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Avg Loss for params {params}: {avg_loss:.2f}\")\n",
    "\n",
    "    # Update the best parameters if the current configuration is better\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_params = params\n",
    "        best_model_state = model.state_dict() \n",
    "\n",
    "print('\\n')\n",
    "print(f\"Best Params: lr={best_params[0]}, weight_decay={best_params[1]}\")\n",
    "print(f\"Best Loss: {best_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'fc1.weight': tensor([[ 2.7005e+05, -1.8811e+05, -4.1405e+05, -2.4953e+05,  5.7674e+03,\n",
      "         -2.3878e+05,  3.2838e+05,  2.5743e+04,  2.5737e+04, -5.2785e+04,\n",
      "          3.7949e+04],\n",
      "        [-5.4235e+02,  5.5079e+03,  8.3578e+03,  5.0522e+03, -8.4918e+02,\n",
      "          4.8349e+03, -5.0199e+03, -5.1970e+02, -5.1934e+02,  1.0653e+03,\n",
      "         -7.6577e+02],\n",
      "        [-2.2678e+03,  4.2710e+03, -1.1619e+04,  3.3713e+03,  6.7128e+03,\n",
      "          3.2257e+03,  2.0274e+03, -3.4145e+02, -3.5254e+02,  7.1810e+02,\n",
      "         -5.2058e+02],\n",
      "        [-6.1468e+03,  5.7238e+04,  8.7687e+04,  5.9508e+04, -4.0708e+04,\n",
      "          5.6942e+04, -1.2700e+04, -6.1380e+03, -6.1378e+03,  1.2588e+04,\n",
      "         -9.0502e+03],\n",
      "        [ 2.8840e+04, -1.6336e+04, -4.8248e+04, -3.2751e+04,  2.9720e+03,\n",
      "         -3.1339e+04,  4.2363e+04,  3.3834e+03,  3.3760e+03, -6.9270e+03,\n",
      "          4.9777e+03]], device='cuda:0'), 'fc1.bias': tensor([-474329.5000,    9571.5361,    6508.9658,  113119.8906,  -62217.1367],\n",
      "       device='cuda:0'), 'fc2.weight': tensor([[ 4.1469e+04,  2.6356e+06, -4.3326e+02,  3.4618e+06,  4.4125e+03],\n",
      "        [-9.2379e+02,  2.2314e+02, -7.2296e+02,  3.8697e+02, -8.8259e+02],\n",
      "        [-1.8609e+02, -1.8592e+02, -1.7606e+02, -1.8781e+02, -1.8548e+02]],\n",
      "       device='cuda:0'), 'fc2.bias': tensor([ 3.5360e+06, -8.2168e+02, -1.6767e+02], device='cuda:0'), 'fc3.weight': tensor([[6.3739e+04, 1.4637e+04, 5.1580e+00]], device='cuda:0'), 'fc3.bias': tensor([64331.0391], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (MSE): 4087500544.00, Test MAE: 52371.81\n"
     ]
    }
   ],
   "source": [
    "best_model = RegressionNN(input_size=X_test.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "best_model.load_state_dict(best_model_state)\n",
    "best_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = best_model(X_test_tensor)\n",
    "    test_mse = criterion(y_pred_tensor, y_test_tensor)\n",
    "    test_mae = mean_absolute_error(y_pred_tensor.cpu(), y_test_tensor.cpu())\n",
    "print(f\"Test loss (MSE): {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set: tensor([[128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875],\n",
      "        [128070.1875]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions on test set:\", y_pred_tensor[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Looking our prediction results seems that we got stuck in a local minima ...\n",
    "\n",
    "... as we already saw in scratch implementation of Neural Network for Regression when target variable has high magnitude values can have a more difficult training resulting in this type of problem\n",
    "\n",
    "So, now let's try to repeat the same we did above but with normalized target variable ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalize also the target variable\n",
    "target_scaler = StandardScaler()\n",
    "y_train_std = target_scaler.fit_transform(y_train)\n",
    "y_train_std = torch.from_numpy(y_train_std).float().to(device) # convert it into tensor\n",
    "# we leave y-test as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9866],\n",
       "        [-1.8979],\n",
       "        [ 0.6032],\n",
       "        ...,\n",
       "        [ 1.5961],\n",
       "        [ 0.0723],\n",
       "        [-0.8901]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (MSE): 2378938368.00, Test MAE: 37803.72\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RegressionNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train_std = model(X_train_tensor)\n",
    "    loss = criterion(preds_train_std, y_train_std)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Prediction \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test_std = model(X_test_tensor)\n",
    "    # De-normalized predictions (to do it we need first to detach and transform them in numpy arrays)\n",
    "    preds_test_numpy = target_scaler.inverse_transform(preds_test_std.detach().cpu().numpy())\n",
    "    preds_test = torch.from_numpy(preds_test_numpy).float().to(device) # convert back to tensor\n",
    "    test_mse = criterion(preds_test, y_test_tensor)\n",
    "    test_mae = mean_absolute_error(preds_test.cpu(), y_test_tensor.cpu())\n",
    "print(f\"Test loss (MSE): {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[178183.8906],\n",
      "        [137942.5469],\n",
      "        [146671.4062],\n",
      "        [166703.4375],\n",
      "        [177999.8906],\n",
      "        [149255.2656],\n",
      "        [163394.2969],\n",
      "        [130299.7656],\n",
      "        [177027.2500],\n",
      "        [121461.8906]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(preds_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the target normalization approach, we can now see that the model is demonstrating significant improvements in  **prediction diversity**. Moreover, we have an imporovement also in performance metrics:\n",
    "\n",
    "- Without normalizing target variable we got:\n",
    "    - Test loss (MSE): 4087500544.00, Test MAE: 52371.81\n",
    "\n",
    "- While normalizing target variable we got:\n",
    "    - Test loss (MSE): 2378938368.00, Test MAE: 37803.72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
