{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - NN for Regression\n",
    "\n",
    "This dataset encompasses details about various workers and their corresponding employment levels, featuring a diverse set of attributes ranging from categorical to continuous. Initialize the data loading process using the suitable Pandas function, and meticulously inspect for any instances of null or duplicated data. Specifically focusing on the **“salary in usd”** feature, identify and eliminate outliers while devising a strategy to address any missing values.\n",
    "\n",
    "Then, use any method you like to encode the categorical features, namely **“work year”, “experience level”, “employment type”, “job title”, “employee residence”, “remote ratio”, “company location”, and “company size”**. You may consider to employ the sklearn LabelEncoder class<sup>1</sup>.\n",
    "\n",
    "Following the preprocessing steps, normalize the dataset utilizing the z-score technique to ensure consistent scaling across features. Subsequently, construct a neural network using **PyTorch**, incorporating **2 hidden layers with 5 and 3 neurons**, respectively. Carefully select an appropriate learning rate and normalization value for optimal model training.\n",
    "\n",
    "Furthermore, assess the model’s performance using a relevant evaluation metric, ensuring a comprehensive understanding of its effectiveness in handling the given employment dataset. Finally, find the best hyperparameter combination (namely **lr** and **weight decay**) using both the **Grid Search** and the **k-fold cross validation** methods.\n",
    "\n",
    "<sup>1</sup>https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>80000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>85847</td>\n",
       "      <td>ES</td>\n",
       "      <td>100</td>\n",
       "      <td>ES</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>30000</td>\n",
       "      <td>USD</td>\n",
       "      <td>30000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>25500</td>\n",
       "      <td>USD</td>\n",
       "      <td>25500</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>175000</td>\n",
       "      <td>USD</td>\n",
       "      <td>175000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000</td>\n",
       "      <td>USD</td>\n",
       "      <td>120000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>412000</td>\n",
       "      <td>USD</td>\n",
       "      <td>412000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>2021</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>151000</td>\n",
       "      <td>USD</td>\n",
       "      <td>151000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>2020</td>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>105000</td>\n",
       "      <td>USD</td>\n",
       "      <td>105000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>2020</td>\n",
       "      <td>EN</td>\n",
       "      <td>CT</td>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>100000</td>\n",
       "      <td>USD</td>\n",
       "      <td>100000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>2021</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>7000000</td>\n",
       "      <td>INR</td>\n",
       "      <td>94665</td>\n",
       "      <td>IN</td>\n",
       "      <td>50</td>\n",
       "      <td>IN</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year experience_level employment_type                 job_title  \\\n",
       "0          2023               SE              FT  Principal Data Scientist   \n",
       "1          2023               MI              CT               ML Engineer   \n",
       "2          2023               MI              CT               ML Engineer   \n",
       "3          2023               SE              FT            Data Scientist   \n",
       "4          2023               SE              FT            Data Scientist   \n",
       "...         ...              ...             ...                       ...   \n",
       "3750       2020               SE              FT            Data Scientist   \n",
       "3751       2021               MI              FT  Principal Data Scientist   \n",
       "3752       2020               EN              FT            Data Scientist   \n",
       "3753       2020               EN              CT     Business Data Analyst   \n",
       "3754       2021               SE              FT      Data Science Manager   \n",
       "\n",
       "       salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0       80000             EUR          85847                 ES           100   \n",
       "1       30000             USD          30000                 US           100   \n",
       "2       25500             USD          25500                 US           100   \n",
       "3      175000             USD         175000                 CA           100   \n",
       "4      120000             USD         120000                 CA           100   \n",
       "...       ...             ...            ...                ...           ...   \n",
       "3750   412000             USD         412000                 US           100   \n",
       "3751   151000             USD         151000                 US           100   \n",
       "3752   105000             USD         105000                 US           100   \n",
       "3753   100000             USD         100000                 US           100   \n",
       "3754  7000000             INR          94665                 IN            50   \n",
       "\n",
       "     company_location company_size  \n",
       "0                  ES            L  \n",
       "1                  US            S  \n",
       "2                  US            S  \n",
       "3                  CA            M  \n",
       "4                  CA            M  \n",
       "...               ...          ...  \n",
       "3750               US            L  \n",
       "3751               US            L  \n",
       "3752               US            S  \n",
       "3753               US            L  \n",
       "3754               IN            L  \n",
       "\n",
       "[3755 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_url = \"./datasets/ds_salaries.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Inspect the dataset\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
      "       'salary_in_usd', 'employee_residence', 'remote_ratio',\n",
      "       'company_location', 'company_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop not useful features\n",
    "df = df.drop(columns=['salary', 'salary_currency'])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "work_year             0\n",
      "experience_level      0\n",
      "employment_type       0\n",
      "job_title             0\n",
      "salary_in_usd         0\n",
      "employee_residence    0\n",
      "remote_ratio          0\n",
      "company_location      0\n",
      "company_size          0\n",
      "dtype: int64\n",
      "Duplicates:  1171\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print('Missing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print('Duplicates: ', df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23256\n"
     ]
    }
   ],
   "source": [
    "# So we found that we do not have missing values, but we have 1171 duplicates. Let's drop them.\n",
    "df = df.drop_duplicates()\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in \"salary in usd\" (even if in this case we don't have any)\n",
    "# For the sake of simplicity, let's fill missing values with the median salary.\n",
    "df['salary_in_usd'] = df['salary_in_usd'].fillna(df['salary_in_usd'].median())\n",
    "\n",
    "# Identify and remove outliers in \"salary in usd\" using IQR\n",
    "alpha = 1.5\n",
    "Q1 = df['salary_in_usd'].quantile(0.25)\n",
    "Q3 = df['salary_in_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - alpha * IQR\n",
    "upper_bound = Q3 + alpha * IQR\n",
    "\n",
    "# we take just entries within the range [lower_bound, upper_bound]\n",
    "df = df[(df['salary_in_usd'] >= lower_bound) & (df['salary_in_usd'] <= upper_bound)]\n",
    "\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical features\n",
    "categorical_features = ['work_year', 'experience_level', 'employment_type', \n",
    "                        'job_title', 'employee_residence', 'remote_ratio', \n",
    "                        'company_location', 'company_size']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to each categorical column\n",
    "for col in categorical_features:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>85847</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>30000</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>25500</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>175000</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>120000</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>165000</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>151000</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>105000</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>100000</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>94665</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2555 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year  experience_level  employment_type  job_title  salary_in_usd  \\\n",
       "0             3                 3                2         83          85847   \n",
       "1             3                 2                0         65          30000   \n",
       "2             3                 2                0         65          25500   \n",
       "3             3                 3                2         46         175000   \n",
       "4             3                 3                2         46         120000   \n",
       "...         ...               ...              ...        ...            ...   \n",
       "3749          1                 3                2         48         165000   \n",
       "3751          1                 2                2         83         151000   \n",
       "3752          0                 0                2         46         105000   \n",
       "3753          0                 0                0         17         100000   \n",
       "3754          1                 3                2         45          94665   \n",
       "\n",
       "      employee_residence  remote_ratio  company_location  company_size  \n",
       "0                     26             2                25             0  \n",
       "1                     74             2                70             2  \n",
       "2                     74             2                70             2  \n",
       "3                     11             2                12             1  \n",
       "4                     11             2                12             1  \n",
       "...                  ...           ...               ...           ...  \n",
       "3749                  74             2                70             0  \n",
       "3751                  74             2                70             0  \n",
       "3752                  74             2                70             2  \n",
       "3753                  74             2                70             0  \n",
       "3754                  38             1                38             0  \n",
       "\n",
       "[2555 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X = df.drop('salary_in_usd', axis=1).values\n",
    "y = df['salary_in_usd'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Features normalization\n",
    "# Normalize the features using Z-score (so, excluding target variable 'salary_in_usd')\n",
    "features_scaler = StandardScaler() # or RobustScaler()\n",
    "X_train_std = features_scaler.fit_transform(X_train)\n",
    "X_test_std = features_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.08053138, -2.43217262,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774805,  2.41820964],\n",
       "       [-1.74054284, -2.43217262,  0.03952792, ..., -0.01676988,\n",
       "        -2.16777496,  0.21748813],\n",
       "       [-0.4005543 ,  0.67169471,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774805,  0.21748813],\n",
       "       ...,\n",
       "       [-1.74054284,  0.67169471,  0.03952792, ...,  1.0219463 ,\n",
       "        -1.09577528, -1.98323338],\n",
       "       [-0.4005543 ,  0.67169471,  0.03952792, ..., -1.05548606,\n",
       "         0.53774805,  0.21748813],\n",
       "       [ 0.93943424,  0.67169471,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774805,  0.21748813]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([105000,  21844, 186000, ...,  54094, 249500, 213580])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train_std)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform y_train and y_test to be column vectors (so they will match output layer of our NN)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105000],\n",
       "       [ 21844],\n",
       "       [186000],\n",
       "       ...,\n",
       "       [ 54094],\n",
       "       [249500],\n",
       "       [213580]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train_std = torch.from_numpy(X_train_std).float().to(device)\n",
    "X_test_std = torch.from_numpy(X_test_std).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_test= torch.from_numpy(y_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0805, -2.4322,  0.0395,  ...,  1.0219,  0.5377,  2.4182],\n",
      "        [-1.7405, -2.4322,  0.0395,  ..., -0.0168, -2.1678,  0.2175],\n",
      "        [-0.4006,  0.6717,  0.0395,  ...,  1.0219,  0.5377,  0.2175],\n",
      "        ...,\n",
      "        [-1.7405,  0.6717,  0.0395,  ...,  1.0219, -1.0958, -1.9832],\n",
      "        [-0.4006,  0.6717,  0.0395,  ..., -1.0555,  0.5377,  0.2175],\n",
      "        [ 0.9394,  0.6717,  0.0395,  ...,  1.0219,  0.5377,  0.2175]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) # Input -> First Hidden Layer\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) # First Hidden Layer layer -> Second Hidden Layer\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size)  # Second Hidden Layer -> Output Layer\n",
    "        self.sigmoid = nn.Sigmoid() # in regression task the sigmoid is not applied at the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = [5, 3]\n",
    "output_size = 1  # Regression task for predicting salary_in_usd\n",
    "\n",
    "model = NN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 10139148288.00000 | Test MSE: 10078524416.00000, Test MAE: 82511.83594\n",
      "Epoch: 200 | Training loss (MSE): 5151995392.00000 | Test MSE: 5215097856.00000, Test MAE: 57558.57422\n",
      "Epoch: 300 | Training loss (MSE): 4151564544.00000 | Test MSE: 4250427648.00000, Test MAE: 52156.82031\n",
      "Epoch: 400 | Training loss (MSE): 3950876928.00000 | Test MSE: 4061814528.00000, Test MAE: 51232.86328\n",
      "Epoch: 500 | Training loss (MSE): 3910618368.00000 | Test MSE: 4026173184.00000, Test MAE: 51065.75781\n",
      "Epoch: 600 | Training loss (MSE): 3902542336.00000 | Test MSE: 4020006656.00000, Test MAE: 51039.69922\n",
      "Epoch: 700 | Training loss (MSE): 3900922368.00000 | Test MSE: 4019209728.00000, Test MAE: 51046.93359\n",
      "Epoch: 800 | Training loss (MSE): 3900597248.00000 | Test MSE: 4019247104.00000, Test MAE: 51061.28906\n",
      "Epoch: 900 | Training loss (MSE): 3900531968.00000 | Test MSE: 4019343104.00000, Test MAE: 51067.71484\n",
      "Epoch: 1000 | Training loss (MSE): 3900518656.00000 | Test MSE: 4019401472.00000, Test MAE: 51070.59375\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train = model(X_train_std)\n",
    "    loss = criterion(preds_train, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # Evaluate the model on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test = model(X_test_std)\n",
    "            test_mse = criterion(preds_test, y_test)\n",
    "            test_mae = mean_absolute_error(preds_test.cpu(), y_test.cpu())\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {loss.item():.5f} | Test MSE: {test_mse.item():.5f}, Test MAE: {test_mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to perform **hyperparameter tuning** using **Grid Search** with **k-fold cross validation** it's efficient to encapsulate the entire process in a single method. This approach allows us to easily repeat the training and evaluation process as needed. Let's create a method to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, X_train_fold, y_train_fold, X_val_fold, y_val_fold, fold, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train and evaluate a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        criterion (torch.nn.Module): The loss function (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training (e.g., Adam, SGD).\n",
    "        X_train (torch.Tensor): Training features.\n",
    "        y_train (torch.Tensor): Training target values.\n",
    "        X_test (torch.Tensor): Test features.\n",
    "        y_test (torch.Tensor): Test target values.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        float: Final test loss (MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        # Forward pass\n",
    "        preds_train = model(X_train_fold)\n",
    "        loss = criterion(preds_train, y_train_fold)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if (epoch+1) % num_epochs == 0: # we print just the last step results\n",
    "            # Evaluate the model on the test data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Compute MSE and MAE for validation data (just for logging)\n",
    "                preds_val = model(X_val_fold)      \n",
    "                val_mse = criterion(preds_val, y_val_fold)\n",
    "                val_mae = mean_absolute_error(preds_val.cpu(), y_val_fold.cpu())\n",
    "            print(f\"Fold: {fold} | Val loss (MSE): {val_mse.item():.5f}, Val MAE: {val_mae:.5f}\")\n",
    "\n",
    "    return val_mse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are ready to actually perform hyperparameter tuning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0805314 , -2.4321725 ,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774804,  2.4182096 ],\n",
       "       [-1.7405429 , -2.4321725 ,  0.03952792, ..., -0.01676988,\n",
       "        -2.167775  ,  0.21748814],\n",
       "       [-0.4005543 ,  0.6716947 ,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774804,  0.21748814],\n",
       "       ...,\n",
       "       [-1.7405429 ,  0.6716947 ,  0.03952792, ...,  1.0219463 ,\n",
       "        -1.0957752 , -1.9832333 ],\n",
       "       [-0.4005543 ,  0.6716947 ,  0.03952792, ..., -1.0554861 ,\n",
       "         0.53774804,  0.21748814],\n",
       "       [ 0.93943423,  0.6716947 ,  0.03952792, ...,  1.0219463 ,\n",
       "         0.53774804,  0.21748814]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[105000.],\n",
       "       [ 21844.],\n",
       "       [186000.],\n",
       "       ...,\n",
       "       [ 54094.],\n",
       "       [249500.],\n",
       "       [213580.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's convert back into numpy arrays (so we can perform the k fold splitting)\n",
    "X_train_std = X_train_std.cpu().numpy()\n",
    "X_test_std = X_test_std.cpu().numpy()\n",
    "y_train = y_train.cpu().numpy()\n",
    "y_test = y_test.cpu().numpy()\n",
    "\n",
    "display(X_train_std)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.001, weight_decay=0.0\n",
      "Fold: 1 | Val loss (MSE): 3728678656.00000, Val MAE: 49153.28906\n",
      "Fold: 2 | Val loss (MSE): 3990404864.00000, Val MAE: 50996.93750\n",
      "Fold: 3 | Val loss (MSE): 3900164608.00000, Val MAE: 51443.54688\n",
      "Fold: 4 | Val loss (MSE): 4009527808.00000, Val MAE: 50209.69141\n",
      "Fold: 5 | Val loss (MSE): 4353251840.00000, Val MAE: 53986.99609\n",
      "Avg Loss for params (0.001, 0.0): 4353251840.00000\n",
      "Testing parameters: lr=0.001, weight_decay=0.01\n",
      "Fold: 1 | Val loss (MSE): 4033252352.00000, Val MAE: 50070.53125\n",
      "Fold: 2 | Val loss (MSE): 3989474816.00000, Val MAE: 50982.12109\n",
      "Fold: 3 | Val loss (MSE): 3899711488.00000, Val MAE: 51443.12891\n",
      "Fold: 4 | Val loss (MSE): 3545121536.00000, Val MAE: 48051.44922\n",
      "Fold: 5 | Val loss (MSE): 4347741696.00000, Val MAE: 53976.51953\n",
      "Avg Loss for params (0.001, 0.01): 4347741696.00000\n",
      "Testing parameters: lr=0.001, weight_decay=0.05\n",
      "Fold: 1 | Val loss (MSE): 3729170944.00000, Val MAE: 49100.88672\n",
      "Fold: 2 | Val loss (MSE): 3984356608.00000, Val MAE: 50879.19922\n",
      "Fold: 3 | Val loss (MSE): 4330327040.00000, Val MAE: 53491.95703\n",
      "Fold: 4 | Val loss (MSE): 3576841472.00000, Val MAE: 48022.73828\n",
      "Fold: 5 | Val loss (MSE): 4348393472.00000, Val MAE: 53960.71094\n",
      "Avg Loss for params (0.001, 0.05): 4348393472.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.0\n",
      "Fold: 1 | Val loss (MSE): 3728978432.00000, Val MAE: 49172.81250\n",
      "Fold: 2 | Val loss (MSE): 3990656000.00000, Val MAE: 51000.80078\n",
      "Fold: 3 | Val loss (MSE): 3898075136.00000, Val MAE: 51442.46484\n",
      "Fold: 4 | Val loss (MSE): 3544119040.00000, Val MAE: 48061.96484\n",
      "Fold: 5 | Val loss (MSE): 4347710464.00000, Val MAE: 53981.55469\n",
      "Avg Loss for params (0.005, 0.0): 4347710464.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.01\n",
      "Fold: 1 | Val loss (MSE): 3728752128.00000, Val MAE: 49159.50000\n",
      "Fold: 2 | Val loss (MSE): 3987184384.00000, Val MAE: 50941.16406\n",
      "Fold: 3 | Val loss (MSE): 3898674944.00000, Val MAE: 51442.35547\n",
      "Fold: 4 | Val loss (MSE): 3545184256.00000, Val MAE: 48050.81641\n",
      "Fold: 5 | Val loss (MSE): 4347746304.00000, Val MAE: 53976.21484\n",
      "Avg Loss for params (0.005, 0.01): 4347746304.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.05\n",
      "Fold: 1 | Val loss (MSE): 3729562624.00000, Val MAE: 49094.89062\n",
      "Fold: 2 | Val loss (MSE): 3985399808.00000, Val MAE: 50904.33203\n",
      "Fold: 3 | Val loss (MSE): 3901985536.00000, Val MAE: 51446.67188\n",
      "Fold: 4 | Val loss (MSE): 3550337536.00000, Val MAE: 48029.32031\n",
      "Fold: 5 | Val loss (MSE): 4348328448.00000, Val MAE: 53961.63281\n",
      "Avg Loss for params (0.005, 0.05): 4348328448.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.0\n",
      "Fold: 1 | Val loss (MSE): 3728978432.00000, Val MAE: 49172.82031\n",
      "Fold: 2 | Val loss (MSE): 3990656768.00000, Val MAE: 51000.81250\n",
      "Fold: 3 | Val loss (MSE): 3898075136.00000, Val MAE: 51442.46484\n",
      "Fold: 4 | Val loss (MSE): 3544119040.00000, Val MAE: 48061.96484\n",
      "Fold: 5 | Val loss (MSE): 4347710464.00000, Val MAE: 53981.55859\n",
      "Avg Loss for params (0.01, 0.0): 4347710464.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.01\n",
      "Fold: 1 | Val loss (MSE): 3728751872.00000, Val MAE: 49159.50781\n",
      "Fold: 2 | Val loss (MSE): 3989408768.00000, Val MAE: 50981.03125\n",
      "Fold: 3 | Val loss (MSE): 3898674944.00000, Val MAE: 51442.35547\n",
      "Fold: 4 | Val loss (MSE): 3545749760.00000, Val MAE: 48045.26953\n",
      "Fold: 5 | Val loss (MSE): 4347728384.00000, Val MAE: 53977.55469\n",
      "Avg Loss for params (0.01, 0.01): 4347728384.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.05\n",
      "Fold: 1 | Val loss (MSE): 3729562624.00000, Val MAE: 49094.89062\n",
      "Fold: 2 | Val loss (MSE): 3985399808.00000, Val MAE: 50904.33203\n",
      "Fold: 3 | Val loss (MSE): 3901985536.00000, Val MAE: 51446.67188\n",
      "Fold: 4 | Val loss (MSE): 3548574976.00000, Val MAE: 48033.90625\n",
      "Fold: 5 | Val loss (MSE): 4350236672.00000, Val MAE: 53970.82422\n",
      "Avg Loss for params (0.01, 0.05): 4350236672.00000\n",
      "\n",
      "\n",
      "Best Params: lr=0.005, weight_decay=0.0\n",
      "Best Loss: 4347710464.00000\n"
     ]
    }
   ],
   "source": [
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'weight_decay': [0.0, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Convert param_grid to all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "# Iterate over all parameter combinations\n",
    "for params in param_combinations:\n",
    "    lr, weight_decay = params\n",
    "\n",
    "    print(f\"Testing parameters: lr={lr}, weight_decay={weight_decay}\")\n",
    "\n",
    "    fold_losses = []\n",
    "    fold=0\n",
    "\n",
    "    # Iteratively consider all fold configurations\n",
    "    for train_index, val_index in kf.split(X_train_std):\n",
    "        fold=fold +1\n",
    "        # Split data into train and validation sets for the current fold\n",
    "        X_train_fold = X_train_std[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        X_val_fold = X_train_std[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Convert data to torch tensors\n",
    "        X_train_fold = torch.from_numpy(X_train_fold).float().to(device)\n",
    "        y_train_fold = torch.from_numpy(y_train_fold).float().to(device)\n",
    "        X_val_fold = torch.from_numpy(X_val_fold).float().to(device)\n",
    "        y_val_fold= torch.from_numpy(y_val_fold).float().to(device)\n",
    "\n",
    "        # Initialize model, optimizer (with the lr and weight decay values to test) and criterion\n",
    "        model = NN(input_size=X_train_fold.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        fold_loss = train_and_evaluate(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            X_train_fold=X_train_fold,\n",
    "            y_train_fold=y_train_fold,\n",
    "            X_val_fold=X_val_fold,\n",
    "            y_val_fold=y_val_fold,\n",
    "            fold=fold\n",
    "        )\n",
    "    fold_losses.append(fold_loss)\n",
    "\n",
    "    # Calculate average loss across all folds\n",
    "    avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Avg Loss for params {params}: {avg_loss:.5f}\")\n",
    "\n",
    "    # Update the best parameters if the current configuration is better\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_params = params\n",
    "\n",
    "print('\\n')\n",
    "print(f\"Best Params: lr={best_params[0]}, weight_decay={best_params[1]}\")\n",
    "print(f\"Best Loss: {best_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once found the best params we can re-train our model with them and test it on the test set\n",
    "torch.manual_seed(42)\n",
    "best_model = NN(input_size, hidden_size, output_size).to(device)\n",
    "best_optimizer = optim.SGD(best_model.parameters(), lr=best_params[0], weight_decay=best_params[1])  # SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert (again) data to torch tensors\n",
    "X_train_std = torch.from_numpy(X_train_std).float().to(device)\n",
    "X_test_std = torch.from_numpy(X_test_std).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_test= torch.from_numpy(y_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 4218264320.00000 | Test MSE: 4306754048.00000, Test MAE: 52463.19531\n",
      "Epoch: 200 | Training loss (MSE): 3906104064.00000 | Test MSE: 4022454016.00000, Test MAE: 51047.90625\n",
      "Epoch: 300 | Training loss (MSE): 3900613888.00000 | Test MSE: 4019236864.00000, Test MAE: 51060.30469\n",
      "Epoch: 400 | Training loss (MSE): 3900517120.00000 | Test MSE: 4019416832.00000, Test MAE: 51071.25781\n",
      "Epoch: 500 | Training loss (MSE): 3900515584.00000 | Test MSE: 4019451136.00000, Test MAE: 51072.70312\n",
      "Epoch: 600 | Training loss (MSE): 3900515328.00000 | Test MSE: 4019456000.00000, Test MAE: 51072.89844\n",
      "Epoch: 700 | Training loss (MSE): 3900515328.00000 | Test MSE: 4019456512.00000, Test MAE: 51072.91797\n",
      "Epoch: 800 | Training loss (MSE): 3900515328.00000 | Test MSE: 4019456512.00000, Test MAE: 51072.91797\n",
      "Epoch: 900 | Training loss (MSE): 3900515328.00000 | Test MSE: 4019456512.00000, Test MAE: 51072.91797\n",
      "Epoch: 1000 | Training loss (MSE): 3900515328.00000 | Test MSE: 4019456512.00000, Test MAE: 51072.91797\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train = best_model(X_train_std)\n",
    "    loss = criterion(preds_train, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    best_optimizer.step()\n",
    "    best_optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # Evaluate the model on the test data\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test = best_model(X_test_std)      \n",
    "            test_mse = criterion(preds_test, y_test)\n",
    "            test_mae = mean_absolute_error(preds_test.cpu(), y_test.cpu())\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {loss.item():.5f} | Test MSE: {test_mse.item():.5f}, Test MAE: {test_mae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Extra\n",
    "Using SGD as optimizer we got nice result ...\n",
    "\n",
    "... however we could obtain better results by normalizing also the target variable.\n",
    "\n",
    "This becuase as we saw in Neural Network for Regression exercise (the one from scratch) when target variable has high magnitude values can have a more difficult training (e.g getting stuck in a local minima).\n",
    "\n",
    "So, now let's try to repeat the same we did above but with normalized target variable ... we will see this will lead to better result (as consequence of a more efficient and stable training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = X_train_std.cpu().numpy()\n",
    "X_test_std = X_test_std.cpu().numpy()\n",
    "y_train = y_train.cpu().numpy()\n",
    "y_test = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalize also the target variable\n",
    "target_scaler = StandardScaler()\n",
    "y_train_std = target_scaler.fit_transform(y_train)\n",
    "# we leave y-test as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4144498],\n",
       "       [-1.7459235],\n",
       "       [ 0.8825025],\n",
       "       ...,\n",
       "       [-1.2295443],\n",
       "       [ 1.8992491],\n",
       "       [ 1.3241068]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train_std = torch.from_numpy(X_train_std).float().to(device)\n",
    "X_test_std = torch.from_numpy(X_test_std).float().to(device)\n",
    "y_train_std = torch.from_numpy(y_train_std).float().to(device)\n",
    "y_test= torch.from_numpy(y_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0805, -2.4322,  0.0395,  ...,  1.0219,  0.5377,  2.4182],\n",
      "        [-1.7405, -2.4322,  0.0395,  ..., -0.0168, -2.1678,  0.2175],\n",
      "        [-0.4006,  0.6717,  0.0395,  ...,  1.0219,  0.5377,  0.2175],\n",
      "        ...,\n",
      "        [-1.7405,  0.6717,  0.0395,  ...,  1.0219, -1.0958, -1.9832],\n",
      "        [-0.4006,  0.6717,  0.0395,  ..., -1.0555,  0.5377,  0.2175],\n",
      "        [ 0.9394,  0.6717,  0.0395,  ...,  1.0219,  0.5377,  0.2175]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "model = NN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # !!! Note in this case we use Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 3922163456.00000 | Test loss (MSE): 4041557760.00000, Test MAE: 51478.83203\n",
      "Epoch: 200 | Training loss (MSE): 3791259392.00000 | Test loss (MSE): 3897159168.00000, Test MAE: 50284.60938\n",
      "Epoch: 300 | Training loss (MSE): 3673942016.00000 | Test loss (MSE): 3764498432.00000, Test MAE: 49373.90625\n",
      "Epoch: 400 | Training loss (MSE): 3494677760.00000 | Test loss (MSE): 3558182144.00000, Test MAE: 47951.54688\n",
      "Epoch: 500 | Training loss (MSE): 3253070848.00000 | Test loss (MSE): 3270491392.00000, Test MAE: 45890.89453\n",
      "Epoch: 600 | Training loss (MSE): 3020109056.00000 | Test loss (MSE): 2981674752.00000, Test MAE: 43787.42188\n",
      "Epoch: 700 | Training loss (MSE): 2872146432.00000 | Test loss (MSE): 2786582272.00000, Test MAE: 42279.50000\n",
      "Epoch: 800 | Training loss (MSE): 2803412480.00000 | Test loss (MSE): 2690657792.00000, Test MAE: 41456.58594\n",
      "Epoch: 900 | Training loss (MSE): 2771198720.00000 | Test loss (MSE): 2653285888.00000, Test MAE: 41058.54688\n",
      "Epoch: 1000 | Training loss (MSE): 2749069568.00000 | Test loss (MSE): 2643042560.00000, Test MAE: 40912.22656\n",
      "Epoch: 1100 | Training loss (MSE): 2729979904.00000 | Test loss (MSE): 2638562048.00000, Test MAE: 40845.37891\n",
      "Epoch: 1200 | Training loss (MSE): 2710535424.00000 | Test loss (MSE): 2633615872.00000, Test MAE: 40802.29688\n",
      "Epoch: 1300 | Training loss (MSE): 2688384768.00000 | Test loss (MSE): 2627960064.00000, Test MAE: 40750.42188\n",
      "Epoch: 1400 | Training loss (MSE): 2663043840.00000 | Test loss (MSE): 2620370688.00000, Test MAE: 40682.73828\n",
      "Epoch: 1500 | Training loss (MSE): 2637948928.00000 | Test loss (MSE): 2611687680.00000, Test MAE: 40584.62500\n",
      "Epoch: 1600 | Training loss (MSE): 2615138304.00000 | Test loss (MSE): 2601766912.00000, Test MAE: 40465.91016\n",
      "Epoch: 1700 | Training loss (MSE): 2594483712.00000 | Test loss (MSE): 2591355392.00000, Test MAE: 40347.24609\n",
      "Epoch: 1800 | Training loss (MSE): 2575826176.00000 | Test loss (MSE): 2580915456.00000, Test MAE: 40228.29688\n",
      "Epoch: 1900 | Training loss (MSE): 2559206912.00000 | Test loss (MSE): 2571130624.00000, Test MAE: 40118.75781\n",
      "Epoch: 2000 | Training loss (MSE): 2544513280.00000 | Test loss (MSE): 2562472192.00000, Test MAE: 40029.04297\n",
      "Epoch: 2100 | Training loss (MSE): 2531478016.00000 | Test loss (MSE): 2554730752.00000, Test MAE: 39956.57031\n",
      "Epoch: 2200 | Training loss (MSE): 2520147968.00000 | Test loss (MSE): 2547714048.00000, Test MAE: 39879.05859\n",
      "Epoch: 2300 | Training loss (MSE): 2510678784.00000 | Test loss (MSE): 2541713664.00000, Test MAE: 39808.82812\n",
      "Epoch: 2400 | Training loss (MSE): 2502988288.00000 | Test loss (MSE): 2536953600.00000, Test MAE: 39756.73828\n",
      "Epoch: 2500 | Training loss (MSE): 2496851712.00000 | Test loss (MSE): 2533346304.00000, Test MAE: 39719.41016\n",
      "Epoch: 2600 | Training loss (MSE): 2491981312.00000 | Test loss (MSE): 2530629376.00000, Test MAE: 39686.16797\n",
      "Epoch: 2700 | Training loss (MSE): 2488077568.00000 | Test loss (MSE): 2528520960.00000, Test MAE: 39654.82422\n",
      "Epoch: 2800 | Training loss (MSE): 2484873728.00000 | Test loss (MSE): 2526774784.00000, Test MAE: 39624.46875\n",
      "Epoch: 2900 | Training loss (MSE): 2482157824.00000 | Test loss (MSE): 2525175040.00000, Test MAE: 39594.78516\n",
      "Epoch: 3000 | Training loss (MSE): 2479765504.00000 | Test loss (MSE): 2523532800.00000, Test MAE: 39565.33984\n",
      "Epoch: 3100 | Training loss (MSE): 2477569024.00000 | Test loss (MSE): 2521701120.00000, Test MAE: 39535.21094\n",
      "Epoch: 3200 | Training loss (MSE): 2475470080.00000 | Test loss (MSE): 2519582464.00000, Test MAE: 39507.15625\n",
      "Epoch: 3300 | Training loss (MSE): 2473395456.00000 | Test loss (MSE): 2517137408.00000, Test MAE: 39480.48438\n",
      "Epoch: 3400 | Training loss (MSE): 2471289600.00000 | Test loss (MSE): 2514376704.00000, Test MAE: 39451.71094\n",
      "Epoch: 3500 | Training loss (MSE): 2469113856.00000 | Test loss (MSE): 2511351040.00000, Test MAE: 39420.82422\n",
      "Epoch: 3600 | Training loss (MSE): 2466844416.00000 | Test loss (MSE): 2508129792.00000, Test MAE: 39389.42969\n",
      "Epoch: 3700 | Training loss (MSE): 2464468992.00000 | Test loss (MSE): 2504773632.00000, Test MAE: 39355.76562\n",
      "Epoch: 3800 | Training loss (MSE): 2461980672.00000 | Test loss (MSE): 2501318656.00000, Test MAE: 39320.23828\n",
      "Epoch: 3900 | Training loss (MSE): 2459368192.00000 | Test loss (MSE): 2497773824.00000, Test MAE: 39283.32812\n",
      "Epoch: 4000 | Training loss (MSE): 2456610304.00000 | Test loss (MSE): 2494134528.00000, Test MAE: 39246.91797\n",
      "Epoch: 4100 | Training loss (MSE): 2453675520.00000 | Test loss (MSE): 2490395136.00000, Test MAE: 39211.89844\n",
      "Epoch: 4200 | Training loss (MSE): 2450530304.00000 | Test loss (MSE): 2486560512.00000, Test MAE: 39176.43750\n",
      "Epoch: 4300 | Training loss (MSE): 2447166720.00000 | Test loss (MSE): 2482652160.00000, Test MAE: 39140.30469\n",
      "Epoch: 4400 | Training loss (MSE): 2443624192.00000 | Test loss (MSE): 2478713344.00000, Test MAE: 39106.32812\n",
      "Epoch: 4500 | Training loss (MSE): 2439944960.00000 | Test loss (MSE): 2474783488.00000, Test MAE: 39071.14844\n",
      "Epoch: 4600 | Training loss (MSE): 2436136960.00000 | Test loss (MSE): 2471236096.00000, Test MAE: 39039.50000\n",
      "Epoch: 4700 | Training loss (MSE): 2432163072.00000 | Test loss (MSE): 2468794112.00000, Test MAE: 39020.28125\n",
      "Epoch: 4800 | Training loss (MSE): 2427890432.00000 | Test loss (MSE): 2467533056.00000, Test MAE: 39021.38281\n",
      "Epoch: 4900 | Training loss (MSE): 2422742528.00000 | Test loss (MSE): 2466254592.00000, Test MAE: 39028.40234\n",
      "Epoch: 5000 | Training loss (MSE): 2413289216.00000 | Test loss (MSE): 2457152256.00000, Test MAE: 38972.85938\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5000 # we also increased the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train = model(X_train_std)\n",
    "    loss = criterion(preds_train, y_train_std)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # De-normalized training loss for logging\n",
    "        # To de-normalize the data we need to detach the data and then transform them back to numpy array. So then as consequence to compute the criterion loss we need to\n",
    "        # reconvert them in tensors\n",
    "        preds_train_denorm = target_scaler.inverse_transform(preds_train.detach().cpu().numpy()) # De-normalized predictions (detach and transform them in numpy arrays)\n",
    "        y_train_denorm = target_scaler.inverse_transform(y_train_std.cpu().numpy())  # De-normalized y_train\n",
    "        preds_train_denorm_tensor = torch.tensor(preds_train_denorm, dtype=torch.float32, device='cpu') # Convert back to tensors from NumPy arrays\n",
    "        y_train_denorm_tensor = torch.tensor(y_train_denorm, dtype=torch.float32, device='cpu')\n",
    "        train_loss = criterion(preds_train_denorm_tensor, y_train_denorm_tensor)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test_norm = model(X_test_std)  # Normalized predictions\n",
    "            preds_test_denorm = target_scaler.inverse_transform(preds_test_norm.cpu().numpy())  # De-normalize predictions\n",
    "            preds_test_denorm_tensor = torch.tensor(preds_test_denorm, dtype=torch.float32, device='cpu')\n",
    "            \n",
    "            y_test_tensor = y_test.clone().detach().cpu().float()  # Detach y_test and move to CPU\n",
    "            \n",
    "            test_loss = criterion(preds_test_denorm_tensor, y_test_tensor)\n",
    "            test_mae = mean_absolute_error(preds_test_denorm_tensor, y_test_tensor)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {train_loss.item():.5f} | Test loss (MSE): {test_loss.item():.5f}, Test MAE: {test_mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that using Adam with the normalized target variable leads to even better results.\n",
    "\n",
    "Now let's encapsulate this entire process in a single method in similar way as we did above, so we can simply call it when we will perform Grid Search with K-fold Cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, target_scaler, X_train_fold, y_train_fold, X_val_fold, y_val_fold, fold, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train and evaluate a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        criterion (torch.nn.Module): The loss function (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training (e.g., Adam, SGD).\n",
    "        X_train (torch.Tensor): Training features.\n",
    "        y_train (torch.Tensor): Training target values.\n",
    "        X_test (torch.Tensor): Test features.\n",
    "        y_test (torch.Tensor): Test target values.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        float: Final test loss (MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_fold, y_train_fold = X_train_fold.to(device), y_train_fold.to(device)\n",
    "    X_val_fold, y_val_fold = X_val_fold.to(device), y_val_fold.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        # Forward pass\n",
    "        preds_train = model(X_train_fold)\n",
    "        loss = criterion(preds_train, y_train_fold)\n",
    "\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if (epoch+1) % num_epochs == 0:\n",
    "            # Evaluate the model on the test data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds_val_norm = model(X_val_fold)\n",
    "                preds_val_denorm = target_scaler.inverse_transform(preds_val_norm.cpu().numpy())\n",
    "                preds_val_denorm_tensor = torch.tensor(preds_val_denorm, dtype=torch.float32, device='cpu')\n",
    "\n",
    "                y_val_denorm = target_scaler.inverse_transform(y_val_fold.cpu().numpy())\n",
    "                y_val_tensor = torch.tensor(y_val_denorm, dtype=torch.float32, device='cpu')\n",
    "                \n",
    "                test_loss = criterion(preds_val_denorm_tensor, y_val_tensor)\n",
    "                test_mae = mean_absolute_error(preds_val_denorm_tensor, y_val_tensor)\n",
    "\n",
    "            print(f\"Fold: {fold} | Test loss (MSE): {test_loss.item():.5f}, Test MAE: {test_mae:.5f}\")\n",
    "\n",
    "    return test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.001, weight_decay=0.0\n",
      "Fold: 1 | Test loss (MSE): 2751747072.00000, Test MAE: 41649.92578\n",
      "Fold: 2 | Test loss (MSE): 2535083008.00000, Test MAE: 39307.72266\n",
      "Fold: 3 | Test loss (MSE): 2341580544.00000, Test MAE: 37815.57031\n",
      "Fold: 4 | Test loss (MSE): 2436936448.00000, Test MAE: 39123.32812\n",
      "Fold: 5 | Test loss (MSE): 2949472000.00000, Test MAE: 41890.58203\n",
      "Avg Loss for params (0.001, 0.0): 2949472000.00000\n",
      "Testing parameters: lr=0.001, weight_decay=0.01\n",
      "Fold: 1 | Test loss (MSE): 2632020736.00000, Test MAE: 40982.26953\n",
      "Fold: 2 | Test loss (MSE): 2669598976.00000, Test MAE: 41286.21094\n",
      "Fold: 3 | Test loss (MSE): 2444236032.00000, Test MAE: 38751.43750\n",
      "Fold: 4 | Test loss (MSE): 2642719232.00000, Test MAE: 40531.98828\n",
      "Fold: 5 | Test loss (MSE): 3183097600.00000, Test MAE: 43934.01562\n",
      "Avg Loss for params (0.001, 0.01): 3183097600.00000\n",
      "Testing parameters: lr=0.001, weight_decay=0.05\n",
      "Fold: 1 | Test loss (MSE): 3728975616.00000, Test MAE: 49172.72266\n",
      "Fold: 2 | Test loss (MSE): 3990607104.00000, Test MAE: 51000.07031\n",
      "Fold: 3 | Test loss (MSE): 3898065152.00000, Test MAE: 51442.47266\n",
      "Fold: 4 | Test loss (MSE): 3544086016.00000, Test MAE: 48062.32812\n",
      "Fold: 5 | Test loss (MSE): 4347709952.00000, Test MAE: 53981.55859\n",
      "Avg Loss for params (0.001, 0.05): 4347709952.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.0\n",
      "Fold: 1 | Test loss (MSE): 2425817344.00000, Test MAE: 39233.00391\n",
      "Fold: 2 | Test loss (MSE): 2454200576.00000, Test MAE: 38694.34766\n",
      "Fold: 3 | Test loss (MSE): 2351566080.00000, Test MAE: 37401.06641\n",
      "Fold: 4 | Test loss (MSE): 2410512384.00000, Test MAE: 38946.01562\n",
      "Fold: 5 | Test loss (MSE): 2997609472.00000, Test MAE: 41222.31250\n",
      "Avg Loss for params (0.005, 0.0): 2997609472.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.01\n",
      "Fold: 1 | Test loss (MSE): 2627544576.00000, Test MAE: 40987.25000\n",
      "Fold: 2 | Test loss (MSE): 2666088448.00000, Test MAE: 41333.02344\n",
      "Fold: 3 | Test loss (MSE): 2440177408.00000, Test MAE: 38715.05078\n",
      "Fold: 4 | Test loss (MSE): 2637446144.00000, Test MAE: 40518.76172\n",
      "Fold: 5 | Test loss (MSE): 3151355136.00000, Test MAE: 43676.16016\n",
      "Avg Loss for params (0.005, 0.01): 3151355136.00000\n",
      "Testing parameters: lr=0.005, weight_decay=0.05\n",
      "Fold: 1 | Test loss (MSE): 3728975616.00000, Test MAE: 49172.72266\n",
      "Fold: 2 | Test loss (MSE): 3990607104.00000, Test MAE: 51000.07031\n",
      "Fold: 3 | Test loss (MSE): 3898065152.00000, Test MAE: 51442.47266\n",
      "Fold: 4 | Test loss (MSE): 3544087552.00000, Test MAE: 48062.30859\n",
      "Fold: 5 | Test loss (MSE): 4347709952.00000, Test MAE: 53981.55859\n",
      "Avg Loss for params (0.005, 0.05): 4347709952.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.0\n",
      "Fold: 1 | Test loss (MSE): 2424476672.00000, Test MAE: 38814.53125\n",
      "Fold: 2 | Test loss (MSE): 2576388352.00000, Test MAE: 39607.35156\n",
      "Fold: 3 | Test loss (MSE): 2242151168.00000, Test MAE: 36780.92578\n",
      "Fold: 4 | Test loss (MSE): 2226652672.00000, Test MAE: 37078.16797\n",
      "Fold: 5 | Test loss (MSE): 2935215104.00000, Test MAE: 40571.75781\n",
      "Avg Loss for params (0.01, 0.0): 2935215104.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.01\n",
      "Fold: 1 | Test loss (MSE): 2626310912.00000, Test MAE: 40951.10156\n",
      "Fold: 2 | Test loss (MSE): 2661625856.00000, Test MAE: 41278.83594\n",
      "Fold: 3 | Test loss (MSE): 2436295168.00000, Test MAE: 38688.18750\n",
      "Fold: 4 | Test loss (MSE): 2650897920.00000, Test MAE: 40613.28516\n",
      "Fold: 5 | Test loss (MSE): 3151381248.00000, Test MAE: 43675.07422\n",
      "Avg Loss for params (0.01, 0.01): 3151381248.00000\n",
      "Testing parameters: lr=0.01, weight_decay=0.05\n",
      "Fold: 1 | Test loss (MSE): 3728976640.00000, Test MAE: 49172.75000\n",
      "Fold: 2 | Test loss (MSE): 3990607872.00000, Test MAE: 51000.07031\n",
      "Fold: 3 | Test loss (MSE): 3898065152.00000, Test MAE: 51442.47266\n",
      "Fold: 4 | Test loss (MSE): 3544087552.00000, Test MAE: 48062.30859\n",
      "Fold: 5 | Test loss (MSE): 4347709952.00000, Test MAE: 53981.55859\n",
      "Avg Loss for params (0.01, 0.05): 4347709952.00000\n",
      "\n",
      "\n",
      "Best Params: lr=0.01, weight_decay=0.0\n",
      "Best Loss: 2935215104.00000\n"
     ]
    }
   ],
   "source": [
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'weight_decay': [0.0, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Convert param_grid to all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "# Iterate over all parameter combinations\n",
    "for params in param_combinations:\n",
    "    lr, weight_decay = params\n",
    "\n",
    "    print(f\"Testing parameters: lr={lr}, weight_decay={weight_decay}\")\n",
    "\n",
    "    fold_losses = []\n",
    "    fold=0\n",
    "\n",
    "    # Iteratively consider all fold configurations\n",
    "    for train_index, val_index in kf.split(X_train_std):\n",
    "        fold=fold +1\n",
    "        # Split data into train and validation sets for the current fold\n",
    "        X_train_fold = X_train_std[train_index]\n",
    "        y_train_fold = y_train_std[train_index]\n",
    "        X_val_fold = X_train_std[val_index]\n",
    "        y_val_fold = y_train_std[val_index]\n",
    "\n",
    "        # Initialize model, optimizer (with the lr and weight decay to test) and criterion\n",
    "        model = NN(input_size=X_train_fold.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        fold_loss = train_and_evaluate(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            target_scaler=target_scaler,\n",
    "            X_train_fold=X_train_fold,\n",
    "            y_train_fold=y_train_fold,\n",
    "            X_val_fold=X_val_fold,\n",
    "            y_val_fold=y_val_fold,\n",
    "            fold=fold,\n",
    "            num_epochs=5000\n",
    "        )\n",
    "    fold_losses.append(fold_loss)\n",
    "\n",
    "    # Calculate average loss across all folds\n",
    "    avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Avg Loss for params {params}: {avg_loss:.5f}\")\n",
    "\n",
    "    # Update the best parameters if the current configuration is better\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_params = params\n",
    "\n",
    "print('\\n')\n",
    "print(f\"Best Params: lr={best_params[0]}, weight_decay={best_params[1]}\")\n",
    "print(f\"Best Loss: {best_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once found the best params we can re-train our model with them and test it on the test set\n",
    "torch.manual_seed(42)\n",
    "best_model = NN(input_size, hidden_size, output_size).to(device)\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_params[0], weight_decay=best_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 2785271040.00000 | Test loss (MSE): 2639118848.00000, Test MAE: 40896.19531\n",
      "Epoch: 200 | Training loss (MSE): 2654635520.00000 | Test loss (MSE): 2600004096.00000, Test MAE: 40363.85547\n",
      "Epoch: 300 | Training loss (MSE): 2512472576.00000 | Test loss (MSE): 2543557632.00000, Test MAE: 39801.81641\n",
      "Epoch: 400 | Training loss (MSE): 2473465856.00000 | Test loss (MSE): 2512959744.00000, Test MAE: 39462.26953\n",
      "Epoch: 500 | Training loss (MSE): 2456041728.00000 | Test loss (MSE): 2496257280.00000, Test MAE: 39236.79297\n",
      "Epoch: 600 | Training loss (MSE): 2441771008.00000 | Test loss (MSE): 2481133824.00000, Test MAE: 39072.59375\n",
      "Epoch: 700 | Training loss (MSE): 2427616256.00000 | Test loss (MSE): 2467489024.00000, Test MAE: 38920.14062\n",
      "Epoch: 800 | Training loss (MSE): 2413174784.00000 | Test loss (MSE): 2455692544.00000, Test MAE: 38775.10547\n",
      "Epoch: 900 | Training loss (MSE): 2356956672.00000 | Test loss (MSE): 2384444928.00000, Test MAE: 38339.56641\n",
      "Epoch: 1000 | Training loss (MSE): 2270453504.00000 | Test loss (MSE): 2367336448.00000, Test MAE: 38256.11719\n",
      "Epoch: 1100 | Training loss (MSE): 2242237952.00000 | Test loss (MSE): 2368007680.00000, Test MAE: 38234.58203\n",
      "Epoch: 1200 | Training loss (MSE): 2229498368.00000 | Test loss (MSE): 2338867200.00000, Test MAE: 38069.48828\n",
      "Epoch: 1300 | Training loss (MSE): 2220529152.00000 | Test loss (MSE): 2329626112.00000, Test MAE: 38017.69922\n",
      "Epoch: 1400 | Training loss (MSE): 2212175360.00000 | Test loss (MSE): 2327520512.00000, Test MAE: 38027.96094\n",
      "Epoch: 1500 | Training loss (MSE): 2202816512.00000 | Test loss (MSE): 2323505920.00000, Test MAE: 38105.19141\n",
      "Epoch: 1600 | Training loss (MSE): 2197467648.00000 | Test loss (MSE): 2322983936.00000, Test MAE: 38123.76562\n",
      "Epoch: 1700 | Training loss (MSE): 2194066944.00000 | Test loss (MSE): 2321464576.00000, Test MAE: 38178.69922\n",
      "Epoch: 1800 | Training loss (MSE): 2191217664.00000 | Test loss (MSE): 2325029632.00000, Test MAE: 38152.61719\n",
      "Epoch: 1900 | Training loss (MSE): 2188871168.00000 | Test loss (MSE): 2322633472.00000, Test MAE: 38212.55469\n",
      "Epoch: 2000 | Training loss (MSE): 2186948096.00000 | Test loss (MSE): 2326455808.00000, Test MAE: 38174.82422\n",
      "Epoch: 2100 | Training loss (MSE): 2185402112.00000 | Test loss (MSE): 2327650816.00000, Test MAE: 38173.78125\n",
      "Epoch: 2200 | Training loss (MSE): 2184186368.00000 | Test loss (MSE): 2326575104.00000, Test MAE: 38183.41797\n",
      "Epoch: 2300 | Training loss (MSE): 2183093504.00000 | Test loss (MSE): 2326082560.00000, Test MAE: 38178.58203\n",
      "Epoch: 2400 | Training loss (MSE): 2182117376.00000 | Test loss (MSE): 2325178880.00000, Test MAE: 38170.49609\n",
      "Epoch: 2500 | Training loss (MSE): 2181160704.00000 | Test loss (MSE): 2324198400.00000, Test MAE: 38158.87891\n",
      "Epoch: 2600 | Training loss (MSE): 2180254976.00000 | Test loss (MSE): 2323145984.00000, Test MAE: 38146.24609\n",
      "Epoch: 2700 | Training loss (MSE): 2179347968.00000 | Test loss (MSE): 2322028288.00000, Test MAE: 38132.99219\n",
      "Epoch: 2800 | Training loss (MSE): 2178499328.00000 | Test loss (MSE): 2321227008.00000, Test MAE: 38118.08984\n",
      "Epoch: 2900 | Training loss (MSE): 2177662464.00000 | Test loss (MSE): 2320075520.00000, Test MAE: 38108.07812\n",
      "Epoch: 3000 | Training loss (MSE): 2176872192.00000 | Test loss (MSE): 2318447104.00000, Test MAE: 38106.98828\n",
      "Epoch: 3100 | Training loss (MSE): 2176118784.00000 | Test loss (MSE): 2318535168.00000, Test MAE: 38088.34766\n",
      "Epoch: 3200 | Training loss (MSE): 2175480576.00000 | Test loss (MSE): 2319288832.00000, Test MAE: 38068.79688\n",
      "Epoch: 3300 | Training loss (MSE): 2174751488.00000 | Test loss (MSE): 2317513216.00000, Test MAE: 38073.98047\n",
      "Epoch: 3400 | Training loss (MSE): 2174157312.00000 | Test loss (MSE): 2316645376.00000, Test MAE: 38076.75391\n",
      "Epoch: 3500 | Training loss (MSE): 2173547264.00000 | Test loss (MSE): 2316954368.00000, Test MAE: 38072.62891\n",
      "Epoch: 3600 | Training loss (MSE): 2173003264.00000 | Test loss (MSE): 2317415936.00000, Test MAE: 38072.08594\n",
      "Epoch: 3700 | Training loss (MSE): 2172488704.00000 | Test loss (MSE): 2316891904.00000, Test MAE: 38085.89453\n",
      "Epoch: 3800 | Training loss (MSE): 2171964416.00000 | Test loss (MSE): 2317104384.00000, Test MAE: 38094.26562\n",
      "Epoch: 3900 | Training loss (MSE): 2171480064.00000 | Test loss (MSE): 2317208576.00000, Test MAE: 38102.89453\n",
      "Epoch: 4000 | Training loss (MSE): 2171005184.00000 | Test loss (MSE): 2317558528.00000, Test MAE: 38109.91016\n",
      "Epoch: 4100 | Training loss (MSE): 2170576896.00000 | Test loss (MSE): 2317347328.00000, Test MAE: 38119.55078\n",
      "Epoch: 4200 | Training loss (MSE): 2170167552.00000 | Test loss (MSE): 2317896704.00000, Test MAE: 38120.85547\n",
      "Epoch: 4300 | Training loss (MSE): 2169793792.00000 | Test loss (MSE): 2318552320.00000, Test MAE: 38120.85938\n",
      "Epoch: 4400 | Training loss (MSE): 2169455104.00000 | Test loss (MSE): 2317918720.00000, Test MAE: 38126.10156\n",
      "Epoch: 4500 | Training loss (MSE): 2169129728.00000 | Test loss (MSE): 2320369920.00000, Test MAE: 38110.24609\n",
      "Epoch: 4600 | Training loss (MSE): 2168833280.00000 | Test loss (MSE): 2317682944.00000, Test MAE: 38127.38672\n",
      "Epoch: 4700 | Training loss (MSE): 2168541952.00000 | Test loss (MSE): 2317785856.00000, Test MAE: 38125.82422\n",
      "Epoch: 4800 | Training loss (MSE): 2168268288.00000 | Test loss (MSE): 2317172480.00000, Test MAE: 38127.96094\n",
      "Epoch: 4900 | Training loss (MSE): 2167995392.00000 | Test loss (MSE): 2316987648.00000, Test MAE: 38126.40625\n",
      "Epoch: 5000 | Training loss (MSE): 2167735296.00000 | Test loss (MSE): 2316571136.00000, Test MAE: 38124.28125\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train = best_model(X_train_std)\n",
    "    loss = criterion(preds_train, y_train_std)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    best_optimizer.step()\n",
    "    best_optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # De-normalized training loss for logging\n",
    "        # To de-normalize the data we need to detach the data and then transform them back to numpy array. So then as consequence to compute the criterion loss we need to\n",
    "        # reconvert them in tensors\n",
    "        preds_train_denorm = target_scaler.inverse_transform(preds_train.detach().cpu().numpy()) # De-normalized predictions (detach and transform them in numpy arrays)\n",
    "        y_train_denorm = target_scaler.inverse_transform(y_train_std.cpu().numpy())  # De-normalized y_train\n",
    "        preds_train_denorm_tensor = torch.tensor(preds_train_denorm, dtype=torch.float32, device='cpu') # Convert back to tensors from NumPy arrays\n",
    "        y_train_denorm_tensor = torch.tensor(y_train_denorm, dtype=torch.float32, device='cpu')\n",
    "        train_loss = criterion(preds_train_denorm_tensor, y_train_denorm_tensor)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test_norm = best_model(X_test_std)  # Normalized predictions\n",
    "            preds_test_denorm = target_scaler.inverse_transform(preds_test_norm.cpu().numpy())  # De-normalize predictions\n",
    "            preds_test_denorm_tensor = torch.tensor(preds_test_denorm, dtype=torch.float32, device='cpu')\n",
    "            \n",
    "            y_test_tensor = y_test.clone().detach().cpu().float()  # Detach y_test and move to CPU\n",
    "            \n",
    "            test_loss = criterion(preds_test_denorm_tensor, y_test_tensor)\n",
    "            test_mae = mean_absolute_error(preds_test_denorm_tensor, y_test_tensor)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {train_loss.item():.5f} | Test loss (MSE): {test_loss.item():.5f}, Test MAE: {test_mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without normalizing target variable (and using SGD) we got:\n",
    "- Best Params: lr=0.005, weight_decay=0.0\n",
    "- MAE on test set = 51072.91797\n",
    "\n",
    "While normalizing target variable (and using Adam) we got:\n",
    "- Best Params: lr=0.01, weight_decay=0.0\n",
    "- MAE on test set = 38124.28125\n",
    "\n",
    "So, in second case we got much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
