{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Regression (Exercise)\n",
    "\n",
    "This dataset encompasses details about various workers and their corresponding employment levels, featuring a diverse set of attributes ranging from categorical to continuous. Initialize the data loading process using the suitable Pandas function, and meticulously inspect for any instances of null or duplicated data. Specifically focusing on the **“salary in usd”** feature, identify and eliminate outliers while devising a strategy to address any missing values.\n",
    "\n",
    "Then, use any method you like to encode the categorical features, namely **“work year”, “experience level”, “employment type”, “job title”, “employee residence”, “remote ratio”, “company location”, and “company size”**. You may consider to employ the sklearn LabelEncoder class<sup>1</sup>.\n",
    "\n",
    "Following the preprocessing steps, normalize the dataset utilizing the z-score technique to ensure consistent scaling across features. Subsequently, construct a neural network using **PyTorch**, incorporating **2 hidden layers with 5 and 3 neurons**, respectively. Carefully select an appropriate learning rate and normalization value for optimal model training.\n",
    "\n",
    "Furthermore, assess the model’s performance using a relevant evaluation metric, ensuring a comprehensive understanding of its effectiveness in handling the given employment dataset. Finally, find the best hyperparameter combination (namely **lr** and **weight decay**) using both the **Grid Search** and the **k-fold cross validation** methods.\n",
    "\n",
    "<sup>1</sup>https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoding of Categorical Features**\n",
    "To apply **standardization**, all features must be numeric. While continuous features are already numerical, categorical features must first be **transformed into numerical values**. This transformation process is called **encoding**.\n",
    "\n",
    "We could use Label Encoding for all categorical variables as the track recoomend, but I can tell you that it's not a good move. LabelEncoder is design to encode target labels with value between 0 and n_classes-1, therefore it make no sense to use it for features encoding. Moreover, when encoding features is important to understand the typology of feature considered, since different typologies lead to [different encodings](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features).\n",
    "\n",
    "Specifically, categorical features fall into two main categories:\n",
    "1. **Ordinal Features**: Have a meaningful order (e.g., education level: *High School < Bachelor's < Master's < PhD*).\n",
    "2. **Nominal Features**: Have no intrinsic order (e.g.company location).\n",
    "\n",
    "**1️⃣ Ordinal Features → Ordinal Encoding**\n",
    "\n",
    "Assigns numerical values based on category order. For example, education levels may be assigned values such as:\n",
    "- High School → 0\n",
    "- Bachelor's → 1\n",
    "- Master's → 2\n",
    "- PhD → 3\n",
    "\n",
    "Since ordinal variables have a meaningful ranking, converting them into ordered numbers preserves that relationship.\n",
    "\n",
    "**2️⃣ Nominal Features → One-Hot Encoding (OHE)**\n",
    "\n",
    "Creates binary columns for each category. For instance, if a feature represents company size with values **S, M, L**, one-hot encoding transforms it into three separate columns:\n",
    "- `S` → [1, 0, 0]\n",
    "- `M` → [0, 1, 0]\n",
    "- `L` → [0, 0, 1]\n",
    "\n",
    "One-hot encoding ensures that categories are represented equally without implying any order.\n",
    "\n",
    "⚠️ **Issue: High Cardinality in Nominal Features**\n",
    "For categorical features with many unique values (e.g., thousands of job titles), one-hot encoding creates too many columns, which could lead to the curse of dimensionality.\n",
    "A better alternative in such a case is **Target Encoding**.\n",
    "\n",
    "**3️⃣ Alternative for High-Cardinality Nominal Features → Target Encoding**\n",
    "\n",
    "Replaces each category with the **mean target value** (e.g., average salary for each job title).\n",
    "- Example: If the average salary for “Software Engineer” is 80,000 USD and for “Data Scientist” is 90,000 USD, we replace:\n",
    "Software Engineer → 80,000\n",
    "Data Scientist → 90,000\n",
    "\n",
    "It solve the one-hot encoding **dimensionality explosion** problem (i.e. when dealing with features that have a large number of unique values) while keeping useful statistical information about the relationship between a category and the target variable.\n",
    "\n",
    "##### **When to Apply Encoding?**\n",
    "Just like standardization, encoding should be computed only on the training set:\n",
    "- Fit and transform on the training data (fit_transform).\n",
    "- Transform only on the test set (transform).\n",
    "\n",
    "⚠️ In general, always rember that the test set should remain **untouched**, mimicking real-world data the model has never seen before.\n",
    "\n",
    "Once all features are numeric (including encoded categorical ones), we can apply Z-score standardization to all features (both the transformed categorical ones and the original continuous numerical features).\n",
    "\n",
    "This ensures that all features are on the same scale, having a mean of 0 and a standard deviation of 1, helping models like neural networks learn efficiently.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Note**: I modified a little the dataset: I deleted some redundant features such as salary and salary_currency since thay are to similar to our target variable and moreover since most are categorical features I added three new continuous features company_revenue_million, avg_working_hours_weekly, industry_demand_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_revenue_million</th>\n",
       "      <th>avg_working_hours_weekly</th>\n",
       "      <th>industry_demand_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>85847</td>\n",
       "      <td>ES</td>\n",
       "      <td>100</td>\n",
       "      <td>ES</td>\n",
       "      <td>L</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>30000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>MI</td>\n",
       "      <td>CT</td>\n",
       "      <td>ML Engineer</td>\n",
       "      <td>25500</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>175000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>120000</td>\n",
       "      <td>CA</td>\n",
       "      <td>100</td>\n",
       "      <td>CA</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>412000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>2021</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>151000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>2020</td>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>105000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>S</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>2020</td>\n",
       "      <td>EN</td>\n",
       "      <td>CT</td>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>100000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>2021</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>94665</td>\n",
       "      <td>IN</td>\n",
       "      <td>50</td>\n",
       "      <td>IN</td>\n",
       "      <td>L</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year experience_level employment_type                 job_title  \\\n",
       "0          2023               SE              FT  Principal Data Scientist   \n",
       "1          2023               MI              CT               ML Engineer   \n",
       "2          2023               MI              CT               ML Engineer   \n",
       "3          2023               SE              FT            Data Scientist   \n",
       "4          2023               SE              FT            Data Scientist   \n",
       "...         ...              ...             ...                       ...   \n",
       "3750       2020               SE              FT            Data Scientist   \n",
       "3751       2021               MI              FT  Principal Data Scientist   \n",
       "3752       2020               EN              FT            Data Scientist   \n",
       "3753       2020               EN              CT     Business Data Analyst   \n",
       "3754       2021               SE              FT      Data Science Manager   \n",
       "\n",
       "      salary_in_usd employee_residence  remote_ratio company_location  \\\n",
       "0             85847                 ES           100               ES   \n",
       "1             30000                 US           100               US   \n",
       "2             25500                 US           100               US   \n",
       "3            175000                 CA           100               CA   \n",
       "4            120000                 CA           100               CA   \n",
       "...             ...                ...           ...              ...   \n",
       "3750         412000                 US           100               US   \n",
       "3751         151000                 US           100               US   \n",
       "3752         105000                 US           100               US   \n",
       "3753         100000                 US           100               US   \n",
       "3754          94665                 IN            50               IN   \n",
       "\n",
       "     company_size  company_revenue_million  avg_working_hours_weekly  \\\n",
       "0               L                     1000                        40   \n",
       "1               S                       50                        30   \n",
       "2               S                       50                        30   \n",
       "3               M                      200                        40   \n",
       "4               M                      200                        40   \n",
       "...           ...                      ...                       ...   \n",
       "3750            L                     1000                        40   \n",
       "3751            L                     1000                        40   \n",
       "3752            S                       50                        40   \n",
       "3753            L                     1000                        30   \n",
       "3754            L                     1000                        40   \n",
       "\n",
       "      industry_demand_score  \n",
       "0                        88  \n",
       "1                        78  \n",
       "2                        78  \n",
       "3                        64  \n",
       "4                        64  \n",
       "...                     ...  \n",
       "3750                     64  \n",
       "3751                     88  \n",
       "3752                     64  \n",
       "3753                     79  \n",
       "3754                     71  \n",
       "\n",
       "[3755 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"./datasets/ds_salaries.csv\")\n",
    "\n",
    "# Drop redundant salary columns\n",
    "df = df.drop(columns=[\"salary\", \"salary_currency\"], errors=\"ignore\")\n",
    "\n",
    "# Generate `company_revenue_million` based on `company_size`\n",
    "company_size_to_revenue = {\"S\": 50, \"M\": 200, \"L\": 1000}  # Example revenue in million USD\n",
    "df[\"company_revenue_million\"] = df[\"company_size\"].map(company_size_to_revenue)\n",
    "\n",
    "# Generate `avg_working_hours_weekly` based on `employment_type`\n",
    "employment_to_hours = {\"FT\": 40, \"PT\": 20, \"CT\": 30, \"FL\": 45}  # Example weekly hours\n",
    "df[\"avg_working_hours_weekly\"] = df[\"employment_type\"].map(employment_to_hours)\n",
    "\n",
    "# Generate `industry_demand_score` (synthetic) based on job title\n",
    "np.random.seed(42)  # For reproducibility\n",
    "job_titles = df[\"job_title\"].unique()\n",
    "job_demand_scores = {job: np.random.randint(50, 100) for job in job_titles}  # Random demand score between 50-100\n",
    "df[\"industry_demand_score\"] = df[\"job_title\"].map(job_demand_scores)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "work_year                   0\n",
      "experience_level            0\n",
      "employment_type             0\n",
      "job_title                   0\n",
      "salary_in_usd               0\n",
      "employee_residence          0\n",
      "remote_ratio                0\n",
      "company_location            0\n",
      "company_size                0\n",
      "company_revenue_million     0\n",
      "avg_working_hours_weekly    0\n",
      "industry_demand_score       0\n",
      "dtype: int64\n",
      "Duplicates:  1171\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print('Missing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print('Duplicates: ', df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31008\n"
     ]
    }
   ],
   "source": [
    "# So we found that we do not have missing values, but we have 1171 duplicates. Let's drop them.\n",
    "df = df.drop_duplicates()\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30660\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in \"salary in usd\" (even if in this case we don't have any)\n",
    "# For the sake of simplicity, let's fill missing values with the median salary.\n",
    "df['salary_in_usd'] = df['salary_in_usd'].fillna(df['salary_in_usd'].median())\n",
    "\n",
    "# Identify and remove outliers in \"salary in usd\" using IQR\n",
    "alpha = 1.5\n",
    "Q1 = df['salary_in_usd'].quantile(0.25)\n",
    "Q3 = df['salary_in_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - alpha * IQR\n",
    "upper_bound = Q3 + alpha * IQR\n",
    "\n",
    "# we take just entries within the range [lower_bound, upper_bound]\n",
    "df = df[(df['salary_in_usd'] >= lower_bound) & (df['salary_in_usd'] <= upper_bound)]\n",
    "\n",
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('salary_in_usd', axis=1)\n",
    "y = df['salary_in_usd']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Features\n",
    "To handle categorical features we will proceed as follows: we first distinguish between ordinal and nominal features. \n",
    "\n",
    "For the **ordinal features** we use a Ordinal Encoder, for the second ones, instead for the **nominal features** if the number of unique values is fewer than 10 we apply **One-Hot Encoding**, otherwise we apply **Target Encoding** (to reduce dimensionality explosion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding features:  ['employment_type']\n",
      "Target encoding features:  ['job_title', 'employee_residence', 'company_location']\n"
     ]
    }
   ],
   "source": [
    "ordinal_features = ['work_year', 'experience_level', 'remote_ratio', 'company_size']\n",
    "nominal_features = ['employment_type', 'job_title', 'employee_residence', 'company_location']\n",
    "\n",
    "one_hot_features = []  # To be filled dynamically\n",
    "target_features = []   # To be filled dynamically \n",
    "\n",
    "# For ordinal features, you can simply use OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[ordinal_features] = ordinal_encoder.fit_transform(X_train[ordinal_features])\n",
    "X_test[ordinal_features] = ordinal_encoder.transform(X_test[ordinal_features])\n",
    "\n",
    "\n",
    "threshold = 10 # max unique values\n",
    "# Let's handle nominal features features\n",
    "for col in nominal_features:\n",
    "    if X_train[col].nunique() <= threshold:\n",
    "        one_hot_features.append(col)  # If number of unique values is below threshold, use one-hot encoding\n",
    "    else:\n",
    "        target_features.append(col)  # Otherwise, use target encoding\n",
    "# Note: Basically here we will one_hot_features and target_features based on the number of unique values\n",
    "\n",
    "print(\"One-hot encoding features: \", one_hot_features)\n",
    "print(\"Target encoding features: \", target_features)\n",
    "\n",
    "# Apply One-Hot Encoding for the `one_hot_features`\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, dtype=float, handle_unknown='ignore')\n",
    "X_train_onehot = onehot_encoder.fit_transform(X_train[one_hot_features])\n",
    "X_test_onehot = onehot_encoder.transform(X_test[one_hot_features])\n",
    "X_train_onehot_df = pd.DataFrame(X_train_onehot, \n",
    "                                 columns=onehot_encoder.get_feature_names_out(one_hot_features), \n",
    "                                 index=X_train.index)\n",
    "X_test_onehot_df = pd.DataFrame(X_test_onehot, \n",
    "                                columns=onehot_encoder.get_feature_names_out(one_hot_features), \n",
    "                                index=X_test.index)\n",
    "X_train = pd.concat([X_train, X_train_onehot_df], axis=1).drop(columns=one_hot_features)\n",
    "X_test = pd.concat([X_test, X_test_onehot_df], axis=1).drop(columns=one_hot_features)\n",
    "\n",
    "# Apply Target Encoding for the `target_features`\n",
    "target_encoder = TargetEncoder()\n",
    "X_train[target_features] = target_encoder.fit_transform(X_train[target_features], y_train)\n",
    "X_test[target_features] = target_encoder.transform(X_test[target_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>job_title</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_revenue_million</th>\n",
       "      <th>avg_working_hours_weekly</th>\n",
       "      <th>industry_demand_score</th>\n",
       "      <th>employment_type_CT</th>\n",
       "      <th>employment_type_FL</th>\n",
       "      <th>employment_type_FT</th>\n",
       "      <th>employment_type_PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133210.021277</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142463.895493</td>\n",
       "      <td>122733.034718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>122733.034718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>129539.722002</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>137034.845214</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>138457.543524</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>158168.601781</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>174758.058342</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133210.021277</td>\n",
       "      <td>40859.436374</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39812.130853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154249.369672</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138457.543524</td>\n",
       "      <td>150683.102547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149179.641026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2044 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      work_year  experience_level      job_title  employee_residence  \\\n",
       "3752        0.0               0.0  133210.021277       150683.102547   \n",
       "3656        1.0               0.0  142463.895493       122733.034718   \n",
       "2964        2.0               3.0  129539.722002       150683.102547   \n",
       "2951        2.0               3.0  137034.845214       150683.102547   \n",
       "3532        1.0               2.0  138457.543524       150683.102547   \n",
       "...         ...               ...            ...                 ...   \n",
       "2513        2.0               2.0  158168.601781       150683.102547   \n",
       "1676        3.0               3.0  174758.058342       150683.102547   \n",
       "1738        1.0               3.0  133210.021277        40859.436374   \n",
       "1960        2.0               3.0  154249.369672       150683.102547   \n",
       "1255        3.0               3.0  138457.543524       150683.102547   \n",
       "\n",
       "      remote_ratio  company_location  company_size  company_revenue_million  \\\n",
       "3752           2.0     149179.641026           2.0                       50   \n",
       "3656           1.0     122733.034718           1.0                      200   \n",
       "2964           2.0     149179.641026           1.0                      200   \n",
       "2951           2.0     149179.641026           1.0                      200   \n",
       "3532           2.0     149179.641026           0.0                     1000   \n",
       "...            ...               ...           ...                      ...   \n",
       "2513           2.0     149179.641026           1.0                      200   \n",
       "1676           2.0     149179.641026           1.0                      200   \n",
       "1738           2.0      39812.130853           0.0                     1000   \n",
       "1960           0.0     149179.641026           1.0                      200   \n",
       "1255           2.0     149179.641026           1.0                      200   \n",
       "\n",
       "      avg_working_hours_weekly  industry_demand_score  employment_type_CT  \\\n",
       "3752                        40                     64                 0.0   \n",
       "3656                        40                     60                 0.0   \n",
       "2964                        40                     77                 0.0   \n",
       "2951                        40                     65                 0.0   \n",
       "3532                        40                     73                 0.0   \n",
       "...                        ...                    ...                 ...   \n",
       "2513                        40                     78                 0.0   \n",
       "1676                        40                     71                 0.0   \n",
       "1738                        40                     64                 0.0   \n",
       "1960                        40                     88                 0.0   \n",
       "1255                        40                     73                 0.0   \n",
       "\n",
       "      employment_type_FL  employment_type_FT  employment_type_PT  \n",
       "3752                 0.0                 1.0                 0.0  \n",
       "3656                 0.0                 1.0                 0.0  \n",
       "2964                 0.0                 1.0                 0.0  \n",
       "2951                 0.0                 1.0                 0.0  \n",
       "3532                 0.0                 1.0                 0.0  \n",
       "...                  ...                 ...                 ...  \n",
       "2513                 0.0                 1.0                 0.0  \n",
       "1676                 0.0                 1.0                 0.0  \n",
       "1738                 0.0                 1.0                 0.0  \n",
       "1960                 0.0                 1.0                 0.0  \n",
       "1255                 0.0                 1.0                 0.0  \n",
       "\n",
       "[2044 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform in numpy array\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now all features present numeric values, so as next step we can apply standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.08053138, -2.43217262,  0.04602072, ..., -0.06650266,\n",
       "         0.11996691, -0.07684733],\n",
       "       [-1.74054284, -2.43217262,  0.60287022, ..., -0.06650266,\n",
       "         0.11996691, -0.07684733],\n",
       "       [-0.4005543 ,  0.67169471, -0.1748386 , ..., -0.06650266,\n",
       "         0.11996691, -0.07684733],\n",
       "       ...,\n",
       "       [-1.74054284,  0.67169471,  0.04602072, ..., -0.06650266,\n",
       "         0.11996691, -0.07684733],\n",
       "       [-0.4005543 ,  0.67169471,  1.31205809, ..., -0.06650266,\n",
       "         0.11996691, -0.07684733],\n",
       "       [ 0.93943424,  0.67169471,  0.36178902, ..., -0.06650266,\n",
       "         0.11996691, -0.07684733]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2044,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2044, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's transform y_train and y_test to be column vectors (so they will match output layer of our NN)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105000],\n",
       "       [ 21844],\n",
       "       [186000],\n",
       "       ...,\n",
       "       [ 54094],\n",
       "       [249500],\n",
       "       [213580]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "y_test_tensor= torch.from_numpy(y_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0805, -2.4322,  0.0460,  ..., -0.0665,  0.1200, -0.0768],\n",
      "        [-1.7405, -2.4322,  0.6029,  ..., -0.0665,  0.1200, -0.0768],\n",
      "        [-0.4006,  0.6717, -0.1748,  ..., -0.0665,  0.1200, -0.0768],\n",
      "        ...,\n",
      "        [-1.7405,  0.6717,  0.0460,  ..., -0.0665,  0.1200, -0.0768],\n",
      "        [-0.4006,  0.6717,  1.3121,  ..., -0.0665,  0.1200, -0.0768],\n",
      "        [ 0.9394,  0.6717,  0.3618,  ..., -0.0665,  0.1200, -0.0768]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0]) # Input -> First Hidden Layer\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1]) # First Hidden Layer layer -> Second Hidden Layer\n",
    "        self.fc3 = nn.Linear(hidden_size[1], output_size)  # Second Hidden Layer -> Output Layer\n",
    "        self.sigmoid = nn.Sigmoid() # In regression task the sigmoid is not applied at the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Training loss (MSE): 11662346240.00 | Test MSE: 11629561856.00, Test MAE: 90166.82\n",
      "Epoch: 200 | Training loss (MSE): 7382539264.00 | Test MSE: 7413563392.00, Test MAE: 69121.98\n",
      "Epoch: 300 | Training loss (MSE): 5462580736.00 | Test MSE: 5528785408.00, Test MAE: 59309.98\n",
      "Epoch: 400 | Training loss (MSE): 4601272320.00 | Test MSE: 4687649792.00, Test MAE: 54604.25\n",
      "Epoch: 500 | Training loss (MSE): 4214881024.00 | Test MSE: 4313249280.00, Test MAE: 52499.35\n",
      "Epoch: 600 | Training loss (MSE): 4041543168.00 | Test MSE: 4147260160.00, Test MAE: 51652.74\n",
      "Epoch: 700 | Training loss (MSE): 3963781888.00 | Test MSE: 4074114816.00, Test MAE: 51289.85\n",
      "Epoch: 800 | Training loss (MSE): 3928897024.00 | Test MSE: 4042183936.00, Test MAE: 51140.65\n",
      "Epoch: 900 | Training loss (MSE): 3913248256.00 | Test MSE: 4028452352.00, Test MAE: 51077.52\n",
      "Epoch: 1000 | Training loss (MSE): 3906227712.00 | Test MSE: 4022688512.00, Test MAE: 51048.78\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = [5, 3]\n",
    "output_size = 1\n",
    "model = NN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # SGD optimizer\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train_tensor = model(X_train_tensor)\n",
    "    loss = criterion(preds_train_tensor, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        # Evaluate the model on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute MSE and MAE for test data (just for logging)\n",
    "            preds_test_tensor = model(X_test_tensor)\n",
    "            test_mse = criterion(preds_test_tensor, y_test_tensor)\n",
    "            test_mae = mean_absolute_error(preds_test_tensor.cpu(), y_test_tensor.cpu()) # here I switch to CPU just because MAE of sklearn is not able to work on GPU\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Training loss (MSE): {loss.item():.2f} | Test MSE: {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to perform **hyperparameter tuning** using **Grid Search** with **k-fold cross validation** it's efficient to encapsulate the entire process in a single method. This approach allows us to easily repeat the training and evaluation process as needed. Let's create a method to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, X_train_fold_tensor, y_train_fold_tensor, X_val_fold_tensor, y_val_fold_tensor, fold, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train and evaluate a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        criterion (torch.nn.Module): The loss function (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training (e.g., Adam, SGD).\n",
    "        X_train_fold_tensor (torch.Tensor): Training features.\n",
    "        y_train_fold_tensor (torch.Tensor): Training target values.\n",
    "        X_val_fold_tensor (torch.Tensor): Test features.\n",
    "        y_val_fold_tensor (torch.Tensor): Test target values.\n",
    "        fold: Fold number considered (used just for logging)\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        float: Final val loss (MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        # Forward pass\n",
    "        preds_train = model(X_train_fold_tensor)\n",
    "        loss = criterion(preds_train, y_train_fold_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if (epoch+1) % num_epochs == 0: # we print just the last step results\n",
    "            # Evaluate the model on the test data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Compute MSE and MAE for validation data (just for logging)\n",
    "                preds_val = model(X_val_fold_tensor)      \n",
    "                val_mse = criterion(preds_val, y_val_fold_tensor)\n",
    "                val_mae = mean_absolute_error(preds_val.cpu(), y_val_fold_tensor.cpu())\n",
    "            print(f\"  Fold: {fold} | Val loss (MSE): {val_mse.item():.2f}, Val MAE: {val_mae:.2f}\")\n",
    "\n",
    "    return val_mse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are ready to actually perform hyperparameter tuning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits.\n",
      "\n",
      "\n",
      "[1/9] Testing Params: lr=0.001, weight_decay=0.0\n",
      "  Fold: 1 | Val loss (MSE): 3731662080.00, Val MAE: 49076.13\n",
      "  Fold: 2 | Val loss (MSE): 3990384384.00, Val MAE: 50996.62\n",
      "  Fold: 3 | Val loss (MSE): 3898186496.00, Val MAE: 51442.36\n",
      "  Fold: 4 | Val loss (MSE): 3545713664.00, Val MAE: 48045.62\n",
      "  Fold: 5 | Val loss (MSE): 4353252864.00, Val MAE: 53987.00\n",
      "Avg Loss for params (0.001, 0.0): 3903839897.60\n",
      "\n",
      "[2/9] Testing Params: lr=0.001, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 3732792576.00, Val MAE: 49069.60\n",
      "  Fold: 2 | Val loss (MSE): 3987496192.00, Val MAE: 50947.20\n",
      "  Fold: 3 | Val loss (MSE): 3912232704.00, Val MAE: 51492.78\n",
      "  Fold: 4 | Val loss (MSE): 3546896640.00, Val MAE: 48038.67\n",
      "  Fold: 5 | Val loss (MSE): 4348322816.00, Val MAE: 53961.72\n",
      "Avg Loss for params (0.001, 0.01): 3905548185.60\n",
      "\n",
      "[3/9] Testing Params: lr=0.001, weight_decay=0.05\n",
      "  Fold: 1 | Val loss (MSE): 3729155328.00, Val MAE: 49101.16\n",
      "  Fold: 2 | Val loss (MSE): 3986313216.00, Val MAE: 50923.97\n",
      "  Fold: 3 | Val loss (MSE): 5089977344.00, Val MAE: 57585.89\n",
      "  Fold: 4 | Val loss (MSE): 3551793664.00, Val MAE: 48027.45\n",
      "  Fold: 5 | Val loss (MSE): 4349550080.00, Val MAE: 53967.26\n",
      "Avg Loss for params (0.001, 0.05): 4141357926.40\n",
      "\n",
      "[4/9] Testing Params: lr=0.005, weight_decay=0.0\n",
      "  Fold: 1 | Val loss (MSE): 3728971264.00, Val MAE: 49172.48\n",
      "  Fold: 2 | Val loss (MSE): 3990656000.00, Val MAE: 51000.81\n",
      "  Fold: 3 | Val loss (MSE): 3898075136.00, Val MAE: 51442.46\n",
      "  Fold: 4 | Val loss (MSE): 3544119296.00, Val MAE: 48061.96\n",
      "  Fold: 5 | Val loss (MSE): 4347710464.00, Val MAE: 53981.55\n",
      "Avg Loss for params (0.005, 0.0): 3901906432.00\n",
      "\n",
      "[5/9] Testing Params: lr=0.005, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 3728584448.00, Val MAE: 49132.66\n",
      "  Fold: 2 | Val loss (MSE): 3988820736.00, Val MAE: 50971.14\n",
      "  Fold: 3 | Val loss (MSE): 3898674944.00, Val MAE: 51442.36\n",
      "  Fold: 4 | Val loss (MSE): 3545183744.00, Val MAE: 48050.82\n",
      "  Fold: 5 | Val loss (MSE): 4347746304.00, Val MAE: 53976.22\n",
      "Avg Loss for params (0.005, 0.01): 3901802035.20\n",
      "\n",
      "[6/9] Testing Params: lr=0.005, weight_decay=0.05\n",
      "  Fold: 1 | Val loss (MSE): 3728790528.00, Val MAE: 49109.27\n",
      "  Fold: 2 | Val loss (MSE): 3986485248.00, Val MAE: 50927.43\n",
      "  Fold: 3 | Val loss (MSE): 3900796416.00, Val MAE: 51444.70\n",
      "  Fold: 4 | Val loss (MSE): 3550336512.00, Val MAE: 48029.32\n",
      "  Fold: 5 | Val loss (MSE): 4348823552.00, Val MAE: 53962.83\n",
      "Avg Loss for params (0.005, 0.05): 3903046451.20\n",
      "\n",
      "[7/9] Testing Params: lr=0.01, weight_decay=0.0\n",
      "  Fold: 1 | Val loss (MSE): 3728978432.00, Val MAE: 49172.82\n",
      "  Fold: 2 | Val loss (MSE): 3990656768.00, Val MAE: 51000.81\n",
      "  Fold: 3 | Val loss (MSE): 3898075136.00, Val MAE: 51442.46\n",
      "  Fold: 4 | Val loss (MSE): 3544119040.00, Val MAE: 48061.96\n",
      "  Fold: 5 | Val loss (MSE): 4347710464.00, Val MAE: 53981.56\n",
      "Avg Loss for params (0.01, 0.0): 3901907968.00\n",
      "\n",
      "[8/9] Testing Params: lr=0.01, weight_decay=0.01\n",
      "  Fold: 1 | Val loss (MSE): 3728674816.00, Val MAE: 49152.86\n",
      "  Fold: 2 | Val loss (MSE): 3987208448.00, Val MAE: 50941.63\n",
      "  Fold: 3 | Val loss (MSE): 3898515968.00, Val MAE: 51442.23\n",
      "  Fold: 4 | Val loss (MSE): 3545750016.00, Val MAE: 48045.27\n",
      "  Fold: 5 | Val loss (MSE): 4347728384.00, Val MAE: 53977.55\n",
      "Avg Loss for params (0.01, 0.01): 3901575526.40\n",
      "\n",
      "[9/9] Testing Params: lr=0.01, weight_decay=0.05\n",
      "  Fold: 1 | Val loss (MSE): 3729562624.00, Val MAE: 49094.89\n",
      "  Fold: 2 | Val loss (MSE): 3983674368.00, Val MAE: 50865.31\n",
      "  Fold: 3 | Val loss (MSE): 3904776704.00, Val MAE: 51456.12\n",
      "  Fold: 4 | Val loss (MSE): 3554263808.00, Val MAE: 48025.06\n",
      "  Fold: 5 | Val loss (MSE): 4350236672.00, Val MAE: 53970.82\n",
      "Avg Loss for params (0.01, 0.05): 3904502835.20\n",
      "\n",
      "\n",
      "Best Params: lr=0.01, weight_decay=0.01\n",
      "Best Loss: 3901575526.40\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "cv_folds = 5 # Number of cross-validation folds\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'weight_decay': [0.0, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Convert param_grid to all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "num_candidates = len(param_combinations)  # Total hyperparameter combinations\n",
    "total_fits = num_candidates * cv_folds  # Total model fits\n",
    "\n",
    "print(f\"Fitting {cv_folds} folds for each of {num_candidates} candidates, totalling {total_fits} fits.\\n\")\n",
    "# Iterate over all parameter combinations\n",
    "for idx, params in enumerate(param_combinations):\n",
    "    lr, weight_decay = params\n",
    "    print(f\"\\n[{idx+1}/{num_candidates}] Testing Params: lr={lr}, weight_decay={weight_decay}\")\n",
    "\n",
    "    fold_losses = []\n",
    "    fold=0\n",
    "\n",
    "    # Iteratively consider all fold configurations\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        fold=fold +1\n",
    "        # Split data into train and validation sets for the current fold\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        X_val_fold = X_train[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Convert data to torch tensors\n",
    "        X_train_fold_tensor = torch.from_numpy(X_train_fold).float().to(device)\n",
    "        y_train_fold_tensor = torch.from_numpy(y_train_fold).float().to(device)\n",
    "        X_val_fold_tensor = torch.from_numpy(X_val_fold).float().to(device)\n",
    "        y_val_fold_tensor = torch.from_numpy(y_val_fold).float().to(device)\n",
    "\n",
    "        # Initialize model, optimizer (with the lr and weight decay values to test) and criterion\n",
    "        model = NN(input_size=X_train_fold_tensor.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        fold_loss = train_and_evaluate(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            X_train_fold_tensor=X_train_fold_tensor,\n",
    "            y_train_fold_tensor=y_train_fold_tensor,\n",
    "            X_val_fold_tensor=X_val_fold_tensor,\n",
    "            y_val_fold_tensor=y_val_fold_tensor,\n",
    "            fold=fold\n",
    "        )\n",
    "        fold_losses.append(fold_loss)\n",
    "\n",
    "    # Calculate average loss across all folds\n",
    "    avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Avg Loss for params {params}: {avg_loss:.2f}\")\n",
    "\n",
    "    # Update the best parameters if the current configuration is better\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_params = params\n",
    "        best_model_state = model.state_dict() \n",
    "\n",
    "print('\\n')\n",
    "print(f\"Best Params: lr={best_params[0]}, weight_decay={best_params[1]}\")\n",
    "print(f\"Best Loss: {best_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'fc1.weight': tensor([[ 3.2630,  4.2425,  3.2691,  5.9602, -1.1488,  5.6662,  1.0755, -2.5103,\n",
      "          1.5990,  0.4973, -0.9156, -1.2397,  1.8618, -1.3080],\n",
      "        [-0.0234,  0.2987,  0.3932,  0.5120, -0.2359,  0.2355, -0.1422,  0.0236,\n",
      "         -0.1347, -0.2210, -0.2303,  0.1349,  0.2842,  0.0750],\n",
      "        [-1.3700, -2.3170, -2.0855, -3.0629,  0.7789, -2.7167, -0.3200,  0.8138,\n",
      "         -0.7747, -0.4725,  0.4107,  0.5042, -1.0545,  0.5950],\n",
      "        [-0.9803, -1.9238, -1.3971, -2.3644,  0.6210, -2.2792,  0.0145,  0.7754,\n",
      "         -0.4749,  0.0255,  0.4254,  0.4524, -0.8065,  0.5522],\n",
      "        [-0.9045, -1.4662, -0.9566, -1.5383,  0.3917, -1.5630, -0.2190,  0.4672,\n",
      "         -0.4319, -0.2208,  0.2516,  0.4395, -0.3424,  0.3803]],\n",
      "       device='cuda:0'), 'fc1.bias': tensor([ 20.6913,   1.0102, -11.2419,  -9.1654,  -6.3112], device='cuda:0'), 'fc2.weight': tensor([[ 16.9218,  15.2643,  16.8741,  17.4984,  16.0235],\n",
      "        [153.8076, 138.1495, 154.0072, 158.1416, 144.5004],\n",
      "        [ 68.6989,  62.0144,  68.4947,  70.4786,  64.2081]], device='cuda:0'), 'fc2.bias': tensor([ 33.4040, 301.6400, 135.2556], device='cuda:0'), 'fc3.weight': tensor([[32765.1680, 32398.4043, 31977.5625]], device='cuda:0'), 'fc3.bias': tensor([33584.8125], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (MSE): 4019319808.00, Test MAE: 51066.43\n"
     ]
    }
   ],
   "source": [
    "best_model = NN(input_size=X_test.shape[1], hidden_size=[5, 3], output_size=1).to(device)\n",
    "best_model.load_state_dict(best_model_state)\n",
    "best_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = best_model(X_test_tensor)\n",
    "    test_mse = criterion(y_pred_tensor, y_test_tensor)\n",
    "    test_mae = mean_absolute_error(y_pred_tensor.cpu(), y_test_tensor.cpu())\n",
    "print(f\"Test loss (MSE): {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set: tensor([[130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453],\n",
      "        [130725.9453]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions on test set:\", y_pred_tensor[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Looking our prediction results seems that we got stuck in a local minima ...\n",
    "\n",
    "... as we already saw in scratch implementation of Neural Network for Regression when target variable has high magnitude values can have a more difficult training resulting in this type of problem\n",
    "\n",
    "So, now let's try to repeat the same we did above but with normalized target variable ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalize also the target variable\n",
    "target_scaler = StandardScaler()\n",
    "y_train_std = target_scaler.fit_transform(y_train)\n",
    "y_train_std = torch.from_numpy(y_train_std).float().to(device) # convert it into tensor\n",
    "# we leave y-test as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4144],\n",
       "        [-1.7459],\n",
       "        [ 0.8825],\n",
       "        ...,\n",
       "        [-1.2295],\n",
       "        [ 1.8992],\n",
       "        [ 1.3241]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (MSE): 2191775232.00, Test MAE: 37095.02\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model\n",
    "model = NN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # !!! Note in this case we use Adam\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    preds_train_std = model(X_train_tensor)\n",
    "    loss = criterion(preds_train_std, y_train_std)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Prediction \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test_std = model(X_test_tensor)\n",
    "    # De-normalized predictions (to do it we need first to detach and transform them in numpy arrays)\n",
    "    preds_test_numpy = target_scaler.inverse_transform(preds_test_std.detach().cpu().numpy())\n",
    "    preds_test = torch.from_numpy(preds_test_numpy).float().to(device) # convert back to tensor\n",
    "    test_mse = criterion(preds_test, y_test_tensor)\n",
    "    test_mae = mean_absolute_error(preds_test.cpu(), y_test_tensor.cpu())\n",
    "print(f\"Test loss (MSE): {test_mse.item():.2f}, Test MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57849.0000],\n",
      "        [173994.0781],\n",
      "        [154895.3594],\n",
      "        [159599.9688],\n",
      "        [165134.2031],\n",
      "        [143111.3125],\n",
      "        [ 69813.7266],\n",
      "        [ 96433.1094],\n",
      "        [146585.8750],\n",
      "        [159540.8438]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(preds_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the target normalization approach, we can now see that the model is demonstrating significant improvements in  **prediction diversity**. Moreover, we have an imporovement also in performance metrics:\n",
    "\n",
    "- Without normalizing target variable we got:\n",
    "    - Test loss (MSE): 4019319808.00, Test MAE: 51066.43\n",
    "\n",
    "- While normalizing target variable we got:\n",
    "    - Test loss (MSE): 2191775232.00, Test MAE: 37095.02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
